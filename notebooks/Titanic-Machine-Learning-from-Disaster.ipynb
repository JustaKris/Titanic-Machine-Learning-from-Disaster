{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba9c16a-7a3d-4fe8-bea9-87a98b7d7722",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Titanic-Machine-Learning-from-Disaster (Python 3.13.3)' requires the ipykernel package.\n",
      "\u001b[1;31mInstall 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'd:/Programming/Repositories/Titanic-Machine-Learning-from-Disaster/.venv/Scripts/python.exe -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Titanic Survival Prediction - Kaggle Competition\n",
    "Author: Kristiyan Bonev\n",
    "Updated: November 2025\n",
    "\n",
    "This notebook provides a comprehensive analysis of the Titanic dataset with:\n",
    "- Exploratory Data Analysis (EDA)\n",
    "- Advanced Feature Engineering\n",
    "- Multiple Model Comparisons with Cross-Validation\n",
    "- Hyperparameter Tuning via GridSearch\n",
    "- Ensemble Methods & Voting Classifiers\n",
    "- SHAP Analysis for Model Explainability\n",
    "- Production-Ready ML Pipeline\n",
    "\n",
    "For production deployment, see scripts/run_training.py\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns \n",
    "import matplotlib.pyplot as plt\n",
    "import shap\n",
    "import sklearn\n",
    "import itertools\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.metrics import (\n",
    "    f1_score, \n",
    "    classification_report,\n",
    "    accuracy_score,\n",
    "    roc_auc_score\n",
    ")\n",
    "\n",
    "# Add project root to path for importing custom modules\n",
    "sys.path.append(str(Path.cwd().parent))\n",
    "\n",
    "# Import notebook-specific utilities\n",
    "from notebooks.utils import config\n",
    "from notebooks.utils.visualization import (\n",
    "    plot_confusion_matrix,\n",
    "    plot_roc_curve,\n",
    "    # plot_feature_importance,\n",
    "    # plot_correlation_heatmap,\n",
    "    # plot_learning_curves,\n",
    "    plot_model_comparison,\n",
    "    plot_cv_score_distribution,\n",
    "    # plot_precision_recall_curve\n",
    ")\n",
    "from notebooks.utils.notebook_helpers import (\n",
    "    optimize_models,\n",
    "    evaluate_models_cv,\n",
    "    get_cv_scores_detailed,\n",
    "    save_model_pickle,\n",
    "    # load_model_pickle,\n",
    "    generate_kaggle_submission\n",
    ")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(config.RANDOM_SEED)\n",
    "\n",
    "# Configure matplotlib and seaborn\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "\n",
    "# Display notebook info\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TITANIC SURVIVAL PREDICTION\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Python version: {sys.version.split()[0]}\")\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"Scikit-learn version: {sklearn.__version__}\")\n",
    "print(\"=\"*60 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "985dc62c-df5c-4275-a935-1f5f25a3787b",
   "metadata": {},
   "source": [
    "## Project Planning\n",
    "\n",
    "Let's outline the main steps of the project:\n",
    "- Understanding the nature of the data .info() .describe()\n",
    "- Exploratory analysis - histograms, boxplots and value counts\n",
    "- Deal with missing data\n",
    "- Check metrics correlation\n",
    "- Themes to look into:\n",
    "  - Level of wealth as a predictor\n",
    "  - Location of cabin as a predictor\n",
    "  - Age\n",
    "  - Title\n",
    "- Feature engineering\n",
    "- Data preprocessing and scaling\n",
    "- Baseline models\n",
    "- Trained models\n",
    "- Comparison."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "586f9c7f",
   "metadata": {},
   "source": [
    "## Configuration & Setup\n",
    "\n",
    "**Reproducibility Settings:**\n",
    "- Random Seed: 42 (set across numpy, random, sklearn)\n",
    "- Cross-Validation: Stratified 5-Fold (handles class imbalance)\n",
    "- Primary Metric: Accuracy (Kaggle competition metric)\n",
    "- Secondary Metric: F1-weighted (better for imbalanced classes)\n",
    "\n",
    "**Class Imbalance Handling:**\n",
    "- âœ“ **Stratified K-Fold CV** - Maintains class distribution across folds\n",
    "- âœ“ **Weighted F1 Score** - Accounts for minority class importance\n",
    "- âœ“ **Comprehensive Metrics** - Precision, Recall, ROC-AUC for full picture\n",
    "\n",
    "**Key Improvements in This Version:**\n",
    "- âœ“ Full reproducibility with fixed random seeds\n",
    "- âœ“ Stratified cross-validation for imbalanced data\n",
    "- âœ“ Comprehensive evaluation metrics (accuracy, F1, ROC-AUC)\n",
    "- âœ“ Advanced visualizations\n",
    "- âœ“ Modular code structure with reusable utilities\n",
    "- âœ“ Modern Python best practices (type hints, docstrings)\n",
    "- âœ“ Feature importance analysis\n",
    "- âœ“ SHAP explainability\n",
    "- âœ“ Production-ready ML pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22bbcd0a-284d-4b6d-8c90-9cc1fd608e60",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "  <h3>Data Dictionary</h3>\n",
    "</div>\n",
    "\n",
    "| Variable  | Definition                | Key                                       |\n",
    "|-----------|---------------------------|-------------------------------------------|\n",
    "| survival  | Survival                  | 0 = No, 1 = Yes                           |\n",
    "| pclass    | Ticket class              | 1 = 1st, 2 = 2nd, 3 = 3rd                 |\n",
    "| sex       | Sex                       |                                           |\n",
    "| Age       | Age in years              |                                           |\n",
    "| sibsp     | # of siblings/spouses     |                                           |\n",
    "| parch     | # of parents/children     |                                           |\n",
    "| ticket    | Ticket number             |                                           |\n",
    "| fare      | Passenger fare            |                                           |\n",
    "| cabin     | Cabin number              |                                           |\n",
    "| embarked  | Port of Embarkation       | C = Cherbourg, Q = Queenstown, S = Southampton |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6288e1b0-9689-4b7e-8a6e-467c96e6439a",
   "metadata": {},
   "source": [
    "### Data import\n",
    "\n",
    "For this analysis, I will be exclusively working with the Training set. The training dataset will be used for validation as well. The final predictions for the Kaggle submissions will of course be done on the provided test dataset as per the competition's rules. I'll be doing the initial data analysis on the combined data which I'll differentiate with the use of a train/test flag in a new column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dccbdb3-14f9-40e8-8632-ab3fc89819ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training and test data using config paths\n",
    "train = pd.read_csv(config.TRAIN_DATA_PATH)\n",
    "test = pd.read_csv(config.TEST_DATA_PATH)\n",
    "\n",
    "train['train_test'] = 1\n",
    "test['train_test'] = 0\n",
    "test['Survived'] = np.NaN\n",
    "all_data = pd.concat([train, test])\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "721b2261-e1db-452b-a574-2f9de882a113",
   "metadata": {},
   "source": [
    "## 1. Data Exploration\n",
    "\n",
    "**For Numeric Data:**\n",
    "   - Histograms to explore distribution;\n",
    "   - Correlation plots;\n",
    "   - Pivot tables to try and understand survival rate across numeric variables;\n",
    "\n",
    "**For Categorical Data:**\n",
    "   - Plotting bar charts to explore balance of classes\n",
    "   - Pivot tables again with the same goal of understanding the relation to survival rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8242cc4-9747-4752-922a-eb6e89f94c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's start with an overview of the train data:\n",
    "train.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc274786-edd2-40ff-b60c-7708621ef790",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listing Numerical columns\n",
    "cols = train.columns\n",
    "num_cols = list(train.select_dtypes('number'))\n",
    "print(num_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4030d8f7-c616-4c87-a848-e542a4eaac56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listing Categorical columns\n",
    "cat_cols = list(set(cols) - set([str(col) for col in num_cols]))\n",
    "print(cat_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef92c5d-889d-4a4d-b4b3-11a3bf344cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A look at feature datatypes\n",
    "train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44975115-744b-4f1b-ad1f-029e39018c23",
   "metadata": {},
   "source": [
    "##### Two colums stand out:\n",
    "- Cabin is mostly null values which will need to be looked into and dealt with\n",
    "- Age has a significant number of null vlaues as well which will also need to be investigated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a594595c-f20e-4d81-ba74-53d671fa824e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# An overview of the central tendencies of the numeric data\n",
    "train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f7021b1-e1f6-4464-b8a7-fc89c277ac39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separating the numeric and categorical columns as each will require a separate approach\n",
    "df_numeric = train[['Age','SibSp','Parch','Fare']]\n",
    "df_categorical = train[['Survived','Pclass','Sex','Ticket','Cabin','Embarked']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d782ba-e588-477c-bd73-2d05447e3355",
   "metadata": {},
   "source": [
    "### 1.1 Numeric Features Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc24ab5-5774-40dd-9eca-2bf06cff32a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a grid of subplots for the numeric columns\n",
    "fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(12, 8))\n",
    "axes = axes.flatten()  # Flatten 2D array to 1D\n",
    "\n",
    "# Plot each histogram in a subplot\n",
    "for i, col in enumerate(df_numeric.columns):\n",
    "    plt.sca(axes[i])\n",
    "    plt.hist(df_numeric[col])\n",
    "    plt.title(col)\n",
    "\n",
    "# Adjust layout for better spacing\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plots\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "281898fa-14e5-4e64-872d-998f87b5f074",
   "metadata": {},
   "source": [
    "We have a normal distribution for the age column. The rest are all good candidates for normalization since they are skewed quite heavily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7366d19-8b41-4dd5-8b02-577edccc7cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A look at the correlation of our numeric columns\n",
    "print(df_numeric.corr())\n",
    "sns.heatmap(df_numeric.corr())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e7f4999-3294-4fe0-8b6b-6449361d68e7",
   "metadata": {},
   "source": [
    "Here we can see that Parch and SibSp has a higher correlation, which generally makes sense since Parents are more likely to travel with their multiple kids and spouses tend to travel together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db667f6-9880-4407-8941-4cc3da512d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A look at survival rate across Age, SibSp, Parch, and Fare \n",
    "pd.pivot_table(\n",
    "    data = train,\n",
    "    values = ['Age', 'SibSp', 'Parch', 'Fare'],\n",
    "    index = 'Survived'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18af71ce-9998-44e1-853a-21cec9838338",
   "metadata": {},
   "source": [
    "There are a few interesting aspects to this:\n",
    "- The average age of survivors is 28, so young people tend to have a better chance;\n",
    "-  People who paid higher fare rates were more likely to survive, more than double. This might be the people traveling in first-clas,.tThus the rich survive. Higher class cabins were higher up on the ship;.-\n",
    "Travelling with parents shows a significant correlation. Well done, parents on the Titanic;e- Having siblings looks to not be an advantage;ing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33154808-907a-48e8-a841-197e9ef7c4f0",
   "metadata": {},
   "source": [
    "### 1.2 Categorical Features Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145e78c3-8833-4e4c-913e-4ba2afed68d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same grid of subplots as the one above but containing barplots with our categorical features\n",
    "fig, axes = plt.subplots(nrows=3, ncols=2, figsize=(12, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Plot each barplot in a subplot\n",
    "for i, col in enumerate(df_categorical.columns):\n",
    "    sns.barplot(x=df_categorical[col].value_counts().index, y=df_categorical[col].value_counts(), ax=axes[i])\n",
    "    axes[i].set_title(col)\n",
    "\n",
    "# Adjust layout for better spacing\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plots\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c29491cc-5074-44e6-9dc3-16cc6a24904d",
   "metadata": {},
   "source": [
    "- Cabin and ticket graphs are very messy and will need to be addressed (feature engineering);\n",
    "- Survived: Most of the people died in the shipwreck, only around 300 people survived. The classes are not balanced which needs to be taken into account;\n",
    "- Pclass: The majority of the people traveling were in 3rd class;\n",
    "- Sex: There were roughly double the number of men on board than women;\n",
    "- Embarked: Most of the passengers boarded the ship from Southampton;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d0bdebf-9f01-456a-aa47-f5b10d62a935",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Survival by Pclass - Enhanced Visualization\n",
    "survival_by_class = pd.crosstab(train['Pclass'], train['Survived'], margins=True)\n",
    "survival_rate = train.groupby('Pclass')['Survived'].agg(['sum', 'count', 'mean'])\n",
    "survival_rate.columns = ['Survived', 'Total', 'Survival Rate']\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Count plot\n",
    "sns.countplot(data=train, x='Pclass', hue='Survived', palette='Set2', ax=ax1)\n",
    "ax1.set_title('Survival Count by Passenger Class', fontsize=14, fontweight='bold')\n",
    "ax1.set_xlabel('Passenger Class', fontsize=12)\n",
    "ax1.set_ylabel('Count', fontsize=12)\n",
    "ax1.legend(title='Survived', labels=['No', 'Yes'])\n",
    "\n",
    "# Survival rate plot\n",
    "survival_rate['Survival Rate'].plot(kind='bar', ax=ax2, color=['#ff9999', '#66b3ff', '#99ff99'], edgecolor='black')\n",
    "ax2.set_title('Survival Rate by Passenger Class', fontsize=14, fontweight='bold')\n",
    "ax2.set_xlabel('Passenger Class', fontsize=12)\n",
    "ax2.set_ylabel('Survival Rate', fontsize=12)\n",
    "ax2.set_xticklabels(['1st Class', '2nd Class', '3rd Class'], rotation=0)\n",
    "ax2.set_ylim([0, 1])\n",
    "ax2.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add percentage labels\n",
    "for i, v in enumerate(survival_rate['Survival Rate']):\n",
    "    ax2.text(i, v + 0.02, f'{v:.1%}', ha='center', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸ“Š Survival Statistics by Passenger Class:\")\n",
    "print(survival_rate.to_string())\n",
    "print(f\"\\nâœ¨ Key Insight: 1st class passengers had a {survival_rate.loc[1, 'Survival Rate']:.1%} survival rate,\")\n",
    "print(f\"   compared to only {survival_rate.loc[3, 'Survival Rate']:.1%} for 3rd class!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d3602f1-4f4c-4495-bd25-6b09d8cba37a",
   "metadata": {},
   "source": [
    "**ðŸ“ˆ Analysis:**\n",
    "\n",
    "The data reveals a stark **class-based disparity** in survival rates:\n",
    "- **1st Class**: Highest survival rate (~63%) - better cabin locations closer to lifeboats\n",
    "- **2nd Class**: Moderate survival rate (~47%) - middle decks\n",
    "- **3rd Class**: Lowest survival rate (~24%) - lower decks, farther from escape routes\n",
    "\n",
    "This supports the \"**women and children first**\" protocol being more effectively implemented in higher classes, and the proximity to lifeboats being a critical factor.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97366705-88db-4f5a-9e3f-69225aa7c377",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Survival by Sex - Enhanced Visualization\n",
    "survival_by_sex = train.groupby('Sex')['Survived'].agg(['sum', 'count', 'mean'])\n",
    "survival_by_sex.columns = ['Survived', 'Total', 'Survival Rate']\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Count plot\n",
    "sns.countplot(data=train, x='Sex', hue='Survived', palette='Set1', ax=ax1)\n",
    "ax1.set_title('Survival Count by Sex', fontsize=14, fontweight='bold')\n",
    "ax1.set_xlabel('Sex', fontsize=12)\n",
    "ax1.set_ylabel('Count', fontsize=12)\n",
    "ax1.legend(title='Survived', labels=['No', 'Yes'])\n",
    "\n",
    "# Survival rate plot\n",
    "survival_by_sex['Survival Rate'].plot(kind='bar', ax=ax2, color=['#ff6b9d', '#4d94ff'], edgecolor='black')\n",
    "ax2.set_title('Survival Rate by Sex', fontsize=14, fontweight='bold')\n",
    "ax2.set_xlabel('Sex', fontsize=12)\n",
    "ax2.set_ylabel('Survival Rate', fontsize=12)\n",
    "ax2.set_xticklabels(['Female', 'Male'], rotation=0)\n",
    "ax2.set_ylim([0, 1])\n",
    "ax2.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add percentage labels\n",
    "for i, (idx, v) in enumerate(survival_by_sex['Survival Rate'].items()):\n",
    "    ax2.text(i, v + 0.02, f'{v:.1%}', ha='center', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸ“Š Survival Statistics by Sex:\")\n",
    "print(survival_by_sex.to_string())\n",
    "print(f\"\\nâœ¨ Key Insight: Female passengers had a {survival_by_sex.loc['female', 'Survival Rate']:.1%} survival rate,\")\n",
    "print(f\"   while male passengers had only {survival_by_sex.loc['male', 'Survival Rate']:.1%}!\")\n",
    "print(f\"   Females were {survival_by_sex.loc['female', 'Survival Rate'] / survival_by_sex.loc['male', 'Survival Rate']:.1f}x more likely to survive.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7197bd8c-c3a6-4ddc-a66a-0b5fd2b00072",
   "metadata": {},
   "source": [
    "**ðŸš¢ Historical Context: \"Women and Children First\"**\n",
    "\n",
    "The dramatic difference in survival rates (74% for women vs. 19% for men) clearly demonstrates the **Birkenhead Drill** protocol in action. This maritime tradition prioritized women and children during evacuations, which was largely followed during the Titanic disaster.\n",
    "\n",
    "The data shows this protocol was strictly enforced, contributing to one of the most significant gender disparities in survival rates in maritime history.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae0387d-659b-4eb3-9f48-bfc53a2d6830",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Survival by Embarked - Enhanced Visualization\n",
    "survival_by_embarked = train.groupby('Embarked')['Survived'].agg(['sum', 'count', 'mean'])\n",
    "survival_by_embarked.columns = ['Survived', 'Total', 'Survival Rate']\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Count plot\n",
    "sns.countplot(data=train, x='Embarked', hue='Survived', palette='Set3', ax=ax1)\n",
    "ax1.set_title('Survival Count by Port of Embarkation', fontsize=14, fontweight='bold')\n",
    "ax1.set_xlabel('Port of Embarkation', fontsize=12)\n",
    "ax1.set_ylabel('Count', fontsize=12)\n",
    "ax1.set_xticklabels(['Cherbourg', 'Queenstown', 'Southampton'])\n",
    "ax1.legend(title='Survived', labels=['No', 'Yes'])\n",
    "\n",
    "# Survival rate plot\n",
    "survival_by_embarked['Survival Rate'].plot(kind='bar', ax=ax2, color=['#ffd966', '#93c47d', '#ff9999'], edgecolor='black')\n",
    "ax2.set_title('Survival Rate by Port of Embarkation', fontsize=14, fontweight='bold')\n",
    "ax2.set_xlabel('Port', fontsize=12)\n",
    "ax2.set_ylabel('Survival Rate', fontsize=12)\n",
    "ax2.set_xticklabels(['Cherbourg', 'Queenstown', 'Southampton'], rotation=0)\n",
    "ax2.set_ylim([0, 1])\n",
    "ax2.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add percentage labels\n",
    "for i, (idx, v) in enumerate(survival_by_embarked['Survival Rate'].items()):\n",
    "    ax2.text(i, v + 0.02, f'{v:.1%}', ha='center', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸ“Š Survival Statistics by Port of Embarkation:\")\n",
    "print(survival_by_embarked.to_string())\n",
    "print(f\"\\nðŸ’¡ Insight: Cherbourg passengers had the highest survival rate at {survival_by_embarked.loc['C', 'Survival Rate']:.1%}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "153549a3-500f-487d-aafd-4c4b045b82e6",
   "metadata": {},
   "source": [
    "**ðŸ¤” Why Does Embarkation Port Matter?**\n",
    "\n",
    "At first glance, the port of embarkation seems irrelevant to survival. However, the higher survival rate for Cherbourg passengers (55%) compared to Southampton (34%) isn't causally related to the port itself.\n",
    "\n",
    "**The Real Factor**: Passenger demographics and ticket class distribution:\n",
    "- Cherbourg had more **1st class passengers** (wealthier travelers)\n",
    "- Southampton had more **3rd class passengers** (emigrants)\n",
    "\n",
    "This demonstrates **confounding variables** in data analysis - Embarkation port correlates with survival, but the actual driver is socioeconomic status (passenger class).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad6bb6e-b8c7-4948-b9d7-54a7dd6dfb06",
   "metadata": {},
   "source": [
    "## 2. Feature Engineering\n",
    "1) Cabin - Simplify cabins (evaluated if cabin letter (cabin_letters) or the purchase of tickets across multiple cabins (cabin_multiple) impacted survival)\n",
    "2) Tickets - Do different ticket types impact survival rates?\n",
    "3) Name - Does a person's title relate to survival rates? I am curious if the captain went down with the ship."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e8c46cb-93f5-4373-8440-52d24fd157bc",
   "metadata": {},
   "source": [
    "### 2.1 Cabin\n",
    "After looking at this, we may want to look at cabin by letter or by number. Let's create some categories for this:\n",
    " - Single letters\n",
    " - Multiple letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81278ef5-1cae-40d1-9fe1-d6c44275e347",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cabin Multiple: Passengers who purchased multiple cabins\n",
    "train['cabin_multiple'] = train.Cabin.apply(lambda x: 0 if pd.isna(x) else len(x.split(' ')))\n",
    "\n",
    "# Visualize distribution and survival impact\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Distribution\n",
    "cabin_counts = train['cabin_multiple'].value_counts().sort_index()\n",
    "ax1.bar(cabin_counts.index, cabin_counts.values, color='steelblue', edgecolor='black')\n",
    "ax1.set_title('Distribution of Cabin Ownership', fontsize=14, fontweight='bold')\n",
    "ax1.set_xlabel('Number of Cabins', fontsize=12)\n",
    "ax1.set_ylabel('Count', fontsize=12)\n",
    "ax1.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Survival rate by cabin multiple\n",
    "survival_by_cabin = train.groupby('cabin_multiple')['Survived'].agg(['sum', 'count', 'mean'])\n",
    "survival_by_cabin.columns = ['Survived', 'Total', 'Survival Rate']\n",
    "\n",
    "survival_by_cabin['Survival Rate'].plot(kind='bar', ax=ax2, color=['#ff9999', '#66b3ff', '#99ff99', '#ffcc99', '#ff99cc'], edgecolor='black')\n",
    "ax2.set_title('Survival Rate by Cabin Ownership', fontsize=14, fontweight='bold')\n",
    "ax2.set_xlabel('Number of Cabins', fontsize=12)\n",
    "ax2.set_ylabel('Survival Rate', fontsize=12)\n",
    "ax2.set_xticklabels(ax2.get_xticklabels(), rotation=0)\n",
    "ax2.set_ylim([0, 1])\n",
    "ax2.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add percentage labels\n",
    "for i, v in enumerate(survival_by_cabin['Survival Rate']):\n",
    "    ax2.text(i, v + 0.02, f'{v:.1%}', ha='center', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸ“Š Cabin Ownership Statistics:\")\n",
    "print(survival_by_cabin.to_string())\n",
    "print(f\"\\nðŸ’¡ Insight: {survival_by_cabin.loc[0, 'Total']} passengers ({survival_by_cabin.loc[0, 'Total']/len(train):.1%}) had no cabin data,\")\n",
    "print(f\"   suggesting lower socioeconomic status. Their survival rate: {survival_by_cabin.loc[0, 'Survival Rate']:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ddf2a9-c6b5-42f4-8dac-61503f9c8836",
   "metadata": {},
   "source": [
    "**ðŸ“Œ Key Finding:**\n",
    "\n",
    "The visualization reveals a strong **wealth-survival correlation**:\n",
    "- **No cabin (0)**: 30% survival rate - majority of passengers, lower class\n",
    "- **Single cabin (1)**: 67% survival rate - wealthier passengers  \n",
    "- **Multiple cabins (2+)**: 70-100% survival rates - elite passengers\n",
    "\n",
    "This engineered feature captures **socioeconomic status** better than Pclass alone, as it distinguishes ultra-wealthy passengers within 1st class.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1698de5d-f300-47f2-9173-b36ae5ff8f58",
   "metadata": {},
   "source": [
    "Next, let us look at the actual letter of the cabin they were in. I would expect that the cabins with the same letter are roughly in the same locations, or on the same floors, and logically if a cabin was near the lifeboats, they had a better chance of survival."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f25cd9-ddde-4808-9524-6ebd52958586",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract cabin letter (indicates deck location on ship)\n",
    "train['cabin_letters'] = train.Cabin.apply(lambda x: str(x)[0])\n",
    "\n",
    "# Visualize cabin letter distribution and survival\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Count plot\n",
    "cabin_counts = train['cabin_letters'].value_counts()\n",
    "ax1.bar(range(len(cabin_counts)), cabin_counts.values, tick_label=cabin_counts.index, color='coral', edgecolor='black')\n",
    "ax1.set_title('Distribution of Cabin Letters (Deck Locations)', fontsize=14, fontweight='bold')\n",
    "ax1.set_xlabel('Cabin Letter (n = no cabin)', fontsize=12)\n",
    "ax1.set_ylabel('Count', fontsize=12)\n",
    "ax1.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Survival rate by cabin letter\n",
    "survival_by_cabin_letter = train.groupby('cabin_letters')['Survived'].agg(['sum', 'count', 'mean']).sort_values('mean', ascending=False)\n",
    "survival_by_cabin_letter.columns = ['Survived', 'Total', 'Survival Rate']\n",
    "\n",
    "colors = plt.cm.RdYlGn(survival_by_cabin_letter['Survival Rate'])\n",
    "survival_by_cabin_letter['Survival Rate'].plot(kind='barh', ax=ax2, color=colors, edgecolor='black')\n",
    "ax2.set_title('Survival Rate by Cabin Letter', fontsize=14, fontweight='bold')\n",
    "ax2.set_xlabel('Survival Rate', fontsize=12)\n",
    "ax2.set_ylabel('Cabin Letter', fontsize=12)\n",
    "ax2.set_xlim([0, 1])\n",
    "ax2.grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Add percentage labels\n",
    "for i, v in enumerate(survival_by_cabin_letter['Survival Rate']):\n",
    "    ax2.text(v + 0.02, i, f'{v:.1%}', va='center', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸ“Š Cabin Letter Survival Statistics:\")\n",
    "print(survival_by_cabin_letter.head(10).to_string())\n",
    "print(f\"\\nðŸš¢ Note: Cabins B, D, E had highest survival rates (upper decks, closer to lifeboats)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5554b47-4bd5-4688-969e-c865a2b706a2",
   "metadata": {},
   "source": [
    "**Decision**: While cabin letters show survival variation, the feature has **too many missing values** (77% null). The `cabin_multiple` feature captures wealth/status more reliably with less missingness."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e1a8e6-dd13-46c3-b0ac-ff73a73e8cba",
   "metadata": {},
   "source": [
    "### 2.2 Ticket\n",
    "\n",
    "Judging by the initial overview and the description of the feature on Kaggle, this feature is likely not going to provide information that is of much use but let's try and have a more detailed look. Let's begin by having a look at the numeric and non-numeric ticket count:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1983d89-0eea-4c30-b702-5148bc90d354",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['numeric_ticket'] = train.Ticket.apply(lambda x: 1 if x.isnumeric() else 0)\n",
    "train['numeric_ticket'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da64b55-beaa-4bbd-be77-d52c6aece9ed",
   "metadata": {},
   "source": [
    "Now let's have a look at the letters in the tickets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f39891c3-cec5-4051-b915-d002ffeb6ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['ticket_letters'] = train.Ticket.apply(lambda x: ''.join(x.split(' ')[:-1]).replace('.', '').replace('/', '').lower() if len(x.split(' ')[:-1]) > 0 else 0)\n",
    "train['ticket_letters'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7596063f-d8db-4542-bdf5-4dc5c24ee8d5",
   "metadata": {},
   "source": [
    "Letters don't see to be of much use for model training. Let's check out the survival rate for numeric vs non-numeric tickets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf12a3c-78f2-4bcc-846a-7d7b788d0bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.pivot_table(\n",
    "    train, \n",
    "    index='Survived',\n",
    "    columns='numeric_ticket',\n",
    "    values='Ticket',\n",
    "    aggfunc='count'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64acb188-ca6c-4b7e-9ba7-e172d1f7f119",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Survival rate across different ticket types \n",
    "pd.pivot_table(\n",
    "    train,\n",
    "    index='Survived',\n",
    "    columns='ticket_letters',\n",
    "    values='Ticket',\n",
    "    aggfunc='count'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21327187-b3ff-411d-98c4-e91c1f7ca4b7",
   "metadata": {},
   "source": [
    "Ticket does not seem to be all that relevant to the task at hand.\n",
    "\n",
    "### 2.3 Name\n",
    "The only aspect of this feature which I am interested in is the title. It will serve a very similar purpose as the Sax feature but might provide some additional detail. Let's pull out the titles as a separate feature (Mr., Ms., Master., etc:)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed71c71f-7c7f-4fdd-9e43-a62136c0ee01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract title from name (Mr., Mrs., Master., etc.)\n",
    "train['name_title'] = train['Name'].apply(lambda x: x.split(',')[1].split('.')[0].strip())\n",
    "\n",
    "# Visualize title distribution and survival\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Count plot - top titles only\n",
    "title_counts = train['name_title'].value_counts()\n",
    "top_titles = title_counts.head(10)\n",
    "ax1.barh(range(len(top_titles)), top_titles.values, tick_label=top_titles.index, color='mediumpurple', edgecolor='black')\n",
    "ax1.set_title('Top 10 Passenger Titles (Distribution)', fontsize=14, fontweight='bold')\n",
    "ax1.set_xlabel('Count', fontsize=12)\n",
    "ax1.set_ylabel('Title', fontsize=12)\n",
    "ax1.grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Survival rate by title - only titles with 5+ passengers\n",
    "survival_by_title = train.groupby('name_title')['Survived'].agg(['sum', 'count', 'mean'])\n",
    "survival_by_title.columns = ['Survived', 'Total', 'Survival Rate']\n",
    "survival_by_title = survival_by_title[survival_by_title['Total'] >= 5].sort_values('Survival Rate', ascending=True)\n",
    "\n",
    "colors = plt.cm.get_cmap('RdYlGn')(survival_by_title['Survival Rate'])\n",
    "survival_by_title['Survival Rate'].plot(kind='barh', ax=ax2, color=colors, edgecolor='black')\n",
    "ax2.set_title('Survival Rate by Title (n â‰¥ 5)', fontsize=14, fontweight='bold')\n",
    "ax2.set_xlabel('Survival Rate', fontsize=12)\n",
    "ax2.set_ylabel('Title', fontsize=12)\n",
    "ax2.set_xlim([0, 1])\n",
    "ax2.grid(axis='x', alpha=0.3)\n",
    "ax2.axvline(x=train['Survived'].mean(), color='red', linestyle='--', linewidth=2, label=f'Overall Rate: {train[\"Survived\"].mean():.1%}')\n",
    "ax2.legend()\n",
    "\n",
    "# Add percentage labels\n",
    "for i, v in enumerate(survival_by_title['Survival Rate']):\n",
    "    ax2.text(v + 0.02, i, f'{v:.0%}', va='center', fontweight='bold', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸ“Š Title Survival Statistics (n â‰¥ 5):\")\n",
    "print(survival_by_title.to_string())\n",
    "print(f\"\\nðŸ‘¤ Insight: 'Mrs' (79%), 'Miss' (70%), 'Master' (58%) had high survival - aligns with 'women & children first'\")\n",
    "print(f\"   'Mr' had only 16% survival rate - adult males were last priority\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d91ee3c-c2a1-4601-889a-b10a4cb28fa2",
   "metadata": {},
   "source": [
    "## 3. Data Preprocessing for Model Training\n",
    "\n",
    "Before training our models, we need to transform the raw data into a format suitable for machine learning:\n",
    "\n",
    "**Key Preprocessing Steps:**\n",
    "1. **Handle Missing Values**: \n",
    "   - Drop rows with null `Embarked` (only 2 instances)\n",
    "   - Impute `Age` and `Fare` with median values\n",
    "\n",
    "2. **Feature Selection**:\n",
    "   - Keep: `Pclass`, `Sex`, `Age`, `SibSp`, `Parch`, `Embarked`\n",
    "   - Add engineered features: `cabin_multiple`, `name_title`, `norm_fare`\n",
    "   - Drop: `PassengerId`, `Name`, `Ticket`, `Cabin` (raw versions)\n",
    "\n",
    "3. **Feature Engineering**:\n",
    "   - `cabin_multiple`: Number of cabins owned (proxy for wealth/status)\n",
    "   - `name_title`: Social title extracted from name (Mr., Mrs., etc.)\n",
    "   - `norm_fare`: Log-transformed fare for better distribution\n",
    "\n",
    "4. **Encoding & Scaling**:\n",
    "   - One-hot encoding for categorical variables\n",
    "   - StandardScaler for numeric features (age, sibsp, parch, fare)\n",
    "\n",
    "**Why These Choices?**\n",
    "- Median imputation is robust to outliers\n",
    "- Log transformation normalizes the skewed fare distribution  \n",
    "- One-hot encoding prevents ordinal assumptions for categories\n",
    "- Scaling ensures all features contribute equally to distance-based models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8162fce9-30a0-43f8-84f6-c67ccb6e2878",
   "metadata": {},
   "source": [
    "### 3.1 Applying Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "766a88f8-3687-4f33-b3ba-fa08c391d5e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up all categorical variables that will be used for both the training and the test sets \n",
    "all_data['cabin_multiple'] = all_data['Cabin'].apply(lambda x: 0 if pd.isna(x) else len(x.split(' ')))\n",
    "all_data['name_title'] = all_data['Name'].apply(lambda x: x.split(',')[1].split('.')[0].strip())\n",
    "\n",
    "# Impute nulls for continuous data \n",
    "print(\"ðŸ”§ Imputation Strategy:\")\n",
    "print(f\"   Age: {all_data['Age'].isna().sum()} missing values â†’ imputing with median ({train['Age'].median():.1f} years)\")\n",
    "print(f\"   Fare: {all_data['Fare'].isna().sum()} missing values â†’ imputing with median (${train['Fare'].median():.2f})\")\n",
    "\n",
    "all_data['Age'] = all_data['Age'].fillna(train['Age'].median())\n",
    "all_data['Fare'] = all_data['Fare'].fillna(train['Fare'].median())\n",
    "\n",
    "# Drop null 'Embarked' rows as they are of no relevance (2 instances in the training set and none in the testing set)\n",
    "embarked_nulls = all_data['Embarked'].isna().sum()\n",
    "all_data.dropna(subset=['Embarked'], inplace=True)\n",
    "print(f\"   Embarked: {embarked_nulls} missing values â†’ dropped rows (minimal impact)\")\n",
    "\n",
    "# Log normalization of fare - visualize before/after transformation\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Before: Original fare distribution (highly skewed)\n",
    "all_data['Fare'].hist(bins=50, ax=ax1, color='salmon', edgecolor='black', alpha=0.7)\n",
    "ax1.set_title('Before: Original Fare Distribution (Right-Skewed)', fontsize=14, fontweight='bold')\n",
    "ax1.set_xlabel('Fare ($)', fontsize=12)\n",
    "ax1.set_ylabel('Frequency', fontsize=12)\n",
    "ax1.axvline(all_data['Fare'].median(), color='red', linestyle='--', linewidth=2, label=f'Median: ${all_data[\"Fare\"].median():.2f}')\n",
    "ax1.legend()\n",
    "ax1.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# After: Log-normalized fare (more normal distribution)\n",
    "all_data['norm_fare'] = np.log(all_data['Fare'] + 1)\n",
    "all_data['norm_fare'].hist(bins=50, ax=ax2, color='lightgreen', edgecolor='black', alpha=0.7)\n",
    "ax2.set_title('After: Log-Normalized Fare (More Normal)', fontsize=14, fontweight='bold')\n",
    "ax2.set_xlabel('log(Fare + 1)', fontsize=12)\n",
    "ax2.set_ylabel('Frequency', fontsize=12)\n",
    "ax2.axvline(all_data['norm_fare'].median(), color='red', linestyle='--', linewidth=2, label=f'Median: {all_data[\"norm_fare\"].median():.2f}')\n",
    "ax2.legend()\n",
    "ax2.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nðŸ“Š Fare Transformation Results:\")\n",
    "print(f\"   Original - Skewness: {all_data['Fare'].skew():.2f}, Range: ${all_data['Fare'].min():.2f} - ${all_data['Fare'].max():.2f}\")\n",
    "print(f\"   Log-Norm - Skewness: {all_data['norm_fare'].skew():.2f}, Range: {all_data['norm_fare'].min():.2f} - {all_data['norm_fare'].max():.2f}\")\n",
    "print(f\"   âœ… Log transformation reduced skewness by {(1 - all_data['norm_fare'].skew()/all_data['Fare'].skew())*100:.1f}%\")\n",
    "\n",
    "# Converting fare to a categorical feature for pd.get_dummies()\n",
    "all_data['Pclass'] = all_data['Pclass'].astype(str)\n",
    "\n",
    "# Creating dummy variables from categories (also can use OneHotEncoder)\n",
    "all_dummies = pd.get_dummies(all_data[['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'norm_fare', 'Embarked', 'cabin_multiple', 'name_title', 'train_test']])\n",
    "\n",
    "print(f\"\\nðŸŽ¯ One-Hot Encoding Results:\")\n",
    "print(f\"   Original features: 10\")\n",
    "print(f\"   After encoding: {all_dummies.shape[1]} features\")\n",
    "print(f\"   Categorical features expanded: Pclass (3), Sex (2), Embarked (3), name_title ({all_data['name_title'].nunique()})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a295443",
   "metadata": {},
   "source": [
    "### 3.2 Advanced Feature Engineering (New!)\n",
    "\n",
    "We've implemented **14+ advanced features** to improve model performance beyond the basic feature engineering above. These features are implemented in `src/features/build_features.py` and are used in the production API.\n",
    "\n",
    "**Advanced Features Include:**\n",
    "\n",
    "1. **Family Features**:\n",
    "   - `family_size`: Total family members aboard (SibSp + Parch + 1)\n",
    "   - `is_alone`: Binary indicator if traveling solo\n",
    "   - `fare_per_person`: Fare split among family members (accounts for group tickets)\n",
    "\n",
    "2. **Title Features**:\n",
    "   - `title_grouped`: Consolidates 17+ rare titles into 5 main categories (Mr, Miss, Mrs, Master, Rare)\n",
    "   - Improves signal-to-noise ratio by grouping low-frequency titles\n",
    "\n",
    "3. **Age Features**:\n",
    "   - `age_group`: Binned into Child (<16), Young Adult (16-32), Adult (32-48), Senior (48+)\n",
    "   - `age_missing`: Flag indicating if age was imputed (missing data can be informative)\n",
    "   - `age_class`: Interaction between age and passenger class\n",
    "\n",
    "4. **Fare Features**:\n",
    "   - `fare_group`: Quartile-based binning (Low/Medium/High/VeryHigh)\n",
    "   - **Fare Inference**: If fare is missing, infers from passenger class with family discount applied\n",
    "\n",
    "5. **Cabin Features**:\n",
    "   - `cabin_known`: Binary flag if cabin was recorded\n",
    "   - `cabin_deck`: Deck letter extracted (A-G, or U for unknown)\n",
    "\n",
    "6. **Ticket Features**:\n",
    "   - `ticket_prefix`: Ticket number prefix (often indicates ticket type)\n",
    "   - `ticket_has_letters`: Binary flag if ticket number contains letters\n",
    "\n",
    "7. **Interaction Features**:\n",
    "   - `sex_pclass`: Interaction between sex and class (e.g., women in 1st class had highest survival)\n",
    "\n",
    "**Let's demonstrate these features:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d2c0be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the advanced feature engineering function\n",
    "import sys\n",
    "sys.path.append('..')  # Add parent directory to path\n",
    "\n",
    "from src.features.build_features import apply_feature_engineering, infer_fare_from_class\n",
    "\n",
    "# Demonstrate Fare Inference\n",
    "print(\"ðŸŽŸï¸ FARE INFERENCE FROM PASSENGER CLASS:\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nWhen fare is missing, we infer it from passenger class medians:\")\n",
    "print(\"\\nðŸ“Š Class-Based Fare Estimates:\")\n",
    "for pclass in [1, 2, 3]:\n",
    "    inferred_fare = infer_fare_from_class(pclass, family_size=1)\n",
    "    print(f\"   Class {pclass}: Â£{inferred_fare:.2f}\")\n",
    "\n",
    "print(\"\\nðŸ‘¨â€ðŸ‘©â€ðŸ‘§â€ðŸ‘¦ Family Discount Applied:\")\n",
    "print(\"   Families of 4+ get 10% discount on inferred fare\")\n",
    "family_fare = infer_fare_from_class(pclass=2, family_size=4)\n",
    "solo_fare = infer_fare_from_class(pclass=2, family_size=1)\n",
    "print(f\"   Solo passenger (Class 2): Â£{solo_fare:.2f}\")\n",
    "print(f\"   Family of 4 (Class 2): Â£{family_fare:.2f} (10% discount)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2647a225",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply Advanced Feature Engineering\n",
    "print(\"ðŸ”¬ ADVANCED FEATURE ENGINEERING COMPARISON:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Basic features (what we had before)\n",
    "basic_features = train.shape[1]\n",
    "print(f\"\\nðŸ“¦ Original Dataset: {basic_features} columns\")\n",
    "print(f\"   Columns: {list(train.columns)}\")\n",
    "\n",
    "# Advanced features\n",
    "df_advanced, num_features, cat_features = apply_feature_engineering(\n",
    "    train.copy(), \n",
    "    include_advanced=True\n",
    ")\n",
    "\n",
    "print(f\"\\nâœ¨ Advanced Features: {df_advanced.shape[1]} columns (+{df_advanced.shape[1] - basic_features} new)\")\n",
    "print(f\"\\nðŸ“Š Feature Breakdown:\")\n",
    "print(f\"   â€¢ Numerical Features ({len(num_features)}): {num_features}\")\n",
    "print(f\"   â€¢ Categorical Features ({len(cat_features)}): {cat_features}\")\n",
    "\n",
    "# Show a sample passenger with all features\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ðŸ‘¤ SAMPLE PASSENGER - ALL FEATURES:\")\n",
    "print(\"=\" * 80)\n",
    "sample_idx = 0\n",
    "sample = df_advanced.iloc[sample_idx]\n",
    "\n",
    "print(f\"\\nðŸŽ« Passenger #{int(sample.get('PassengerId', 0))}\")\n",
    "print(\"\\nðŸ“‹ ORIGINAL FEATURES:\")\n",
    "original_cols = ['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked']\n",
    "for col in original_cols:\n",
    "    if col in sample.index:\n",
    "        print(f\"   {col:15s}: {sample[col]}\")\n",
    "\n",
    "print(\"\\nâœ¨ ENGINEERED FEATURES:\")\n",
    "engineered_cols = [\n",
    "    'family_size', 'is_alone', 'fare_per_person', \n",
    "    'title_grouped', 'age_group', 'age_missing', \n",
    "    'age_class', 'fare_group', 'cabin_known', \n",
    "    'cabin_deck', 'sex_pclass'\n",
    "]\n",
    "for col in engineered_cols:\n",
    "    if col in sample.index:\n",
    "        print(f\"   {col:15s}: {sample[col]}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "062dc4ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the impact of key advanced features\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "fig.suptitle('ðŸ” Advanced Feature Analysis - Survival Patterns', fontsize=16, fontweight='bold', y=0.995)\n",
    "\n",
    "# 1. Family Size vs Survival\n",
    "ax1 = axes[0, 0]\n",
    "family_survival = df_advanced.groupby('family_size')['Survived'].agg(['mean', 'count'])\n",
    "family_survival = family_survival[family_survival['count'] >= 5]  # Filter rare sizes\n",
    "ax1.bar(family_survival.index, family_survival['mean'], color='steelblue', edgecolor='black', alpha=0.8)\n",
    "ax1.set_title('Family Size vs Survival Rate', fontsize=13, fontweight='bold')\n",
    "ax1.set_xlabel('Family Size', fontsize=11)\n",
    "ax1.set_ylabel('Survival Rate', fontsize=11)\n",
    "ax1.axhline(y=df_advanced['Survived'].mean(), color='red', linestyle='--', label=f'Overall: {df_advanced[\"Survived\"].mean():.1%}')\n",
    "ax1.legend()\n",
    "ax1.grid(axis='y', alpha=0.3)\n",
    "for i, (idx, row) in enumerate(family_survival.iterrows()):\n",
    "    ax1.text(idx, row['mean'] + 0.02, f\"{row['mean']:.0%}\\n(n={int(row['count'])})\", ha='center', fontsize=9)\n",
    "\n",
    "# 2. Is Alone vs Survival\n",
    "ax2 = axes[0, 1]\n",
    "alone_survival = df_advanced.groupby('is_alone')['Survived'].mean()\n",
    "colors = ['#2ecc71', '#e74c3c']\n",
    "bars = ax2.bar(['With Family', 'Alone'], alone_survival.values, color=colors, edgecolor='black', alpha=0.8)\n",
    "ax2.set_title('Solo vs Family Travelers', fontsize=13, fontweight='bold')\n",
    "ax2.set_ylabel('Survival Rate', fontsize=11)\n",
    "ax2.set_ylim([0, 1])\n",
    "ax2.grid(axis='y', alpha=0.3)\n",
    "for bar, val in zip(bars, alone_survival.values):\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2, val + 0.03, f'{val:.1%}', ha='center', fontweight='bold', fontsize=11)\n",
    "\n",
    "# 3. Title Grouped vs Survival\n",
    "ax3 = axes[0, 2]\n",
    "title_survival = df_advanced.groupby('title_grouped')['Survived'].agg(['mean', 'count'])\n",
    "title_survival = title_survival.sort_values('mean', ascending=False)\n",
    "ax3.barh(range(len(title_survival)), title_survival['mean'], color='coral', edgecolor='black', alpha=0.8)\n",
    "ax3.set_yticks(range(len(title_survival)))\n",
    "ax3.set_yticklabels(title_survival.index)\n",
    "ax3.set_title('Title vs Survival Rate', fontsize=13, fontweight='bold')\n",
    "ax3.set_xlabel('Survival Rate', fontsize=11)\n",
    "ax3.axvline(x=df_advanced['Survived'].mean(), color='red', linestyle='--', linewidth=2)\n",
    "ax3.grid(axis='x', alpha=0.3)\n",
    "for i, (idx, row) in enumerate(title_survival.iterrows()):\n",
    "    ax3.text(row['mean'] + 0.02, i, f\"{row['mean']:.0%} (n={int(row['count'])})\", va='center', fontsize=9)\n",
    "\n",
    "# 4. Age Group vs Survival\n",
    "ax4 = axes[1, 0]\n",
    "age_group_survival = df_advanced.groupby('age_group')['Survived'].agg(['mean', 'count'])\n",
    "age_group_order = ['Child', 'Young Adult', 'Adult', 'Senior']\n",
    "age_group_survival = age_group_survival.reindex([ag for ag in age_group_order if ag in age_group_survival.index])\n",
    "ax4.bar(range(len(age_group_survival)), age_group_survival['mean'], color='purple', edgecolor='black', alpha=0.8)\n",
    "ax4.set_xticks(range(len(age_group_survival)))\n",
    "ax4.set_xticklabels(age_group_survival.index, rotation=45)\n",
    "ax4.set_title('Age Group vs Survival', fontsize=13, fontweight='bold')\n",
    "ax4.set_ylabel('Survival Rate', fontsize=11)\n",
    "ax4.axhline(y=df_advanced['Survived'].mean(), color='red', linestyle='--', linewidth=2)\n",
    "ax4.grid(axis='y', alpha=0.3)\n",
    "for i, (idx, row) in enumerate(age_group_survival.iterrows()):\n",
    "    ax4.text(i, row['mean'] + 0.02, f\"{row['mean']:.0%}\", ha='center', fontsize=9)\n",
    "\n",
    "# 5. Fare Group vs Survival\n",
    "ax5 = axes[1, 1]\n",
    "fare_group_survival = df_advanced.groupby('fare_group')['Survived'].agg(['mean', 'count'])\n",
    "fare_order = ['Low', 'Medium', 'High', 'VeryHigh']\n",
    "fare_group_survival = fare_group_survival.reindex([fg for fg in fare_order if fg in fare_group_survival.index])\n",
    "ax5.bar(range(len(fare_group_survival)), fare_group_survival['mean'], color='gold', edgecolor='black', alpha=0.8)\n",
    "ax5.set_xticks(range(len(fare_group_survival)))\n",
    "ax5.set_xticklabels(fare_group_survival.index, rotation=45)\n",
    "ax5.set_title('Fare Group vs Survival', fontsize=13, fontweight='bold')\n",
    "ax5.set_ylabel('Survival Rate', fontsize=11)\n",
    "ax5.axhline(y=df_advanced['Survived'].mean(), color='red', linestyle='--', linewidth=2)\n",
    "ax5.grid(axis='y', alpha=0.3)\n",
    "for i, (idx, row) in enumerate(fare_group_survival.iterrows()):\n",
    "    ax5.text(i, row['mean'] + 0.02, f\"{row['mean']:.0%}\", ha='center', fontsize=9)\n",
    "\n",
    "# 6. Sex-Class Interaction vs Survival\n",
    "ax6 = axes[1, 2]\n",
    "sex_class_survival = df_advanced.groupby('sex_pclass')['Survived'].agg(['mean', 'count'])\n",
    "sex_class_survival = sex_class_survival.sort_values('mean', ascending=False)\n",
    "colors_sc = ['#e91e63' if 'female' in idx else '#2196f3' for idx in sex_class_survival.index]\n",
    "ax6.barh(range(len(sex_class_survival)), sex_class_survival['mean'], color=colors_sc, edgecolor='black', alpha=0.8)\n",
    "ax6.set_yticks(range(len(sex_class_survival)))\n",
    "ax6.set_yticklabels(sex_class_survival.index, fontsize=9)\n",
    "ax6.set_title('Sex-Class Interaction vs Survival', fontsize=13, fontweight='bold')\n",
    "ax6.set_xlabel('Survival Rate', fontsize=11)\n",
    "ax6.axvline(x=df_advanced['Survived'].mean(), color='red', linestyle='--', linewidth=2)\n",
    "ax6.grid(axis='x', alpha=0.3)\n",
    "for i, (idx, row) in enumerate(sex_class_survival.iterrows()):\n",
    "    ax6.text(row['mean'] + 0.02, i, f\"{row['mean']:.0%}\", va='center', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸ“Š KEY INSIGHTS FROM ADVANCED FEATURES:\")\n",
    "print(\"=\" * 80)\n",
    "print(\"âœ“ Families of 2-4 had better survival than solo travelers or very large families\")\n",
    "print(\"âœ“ 'Mrs' and 'Miss' titles had 70%+ survival (women & children first policy)\")\n",
    "print(\"âœ“ Children (<16) had highest survival among age groups\")\n",
    "print(\"âœ“ Higher fare groups correlated with better survival (proxy for class/wealth)\")\n",
    "print(\"âœ“ Female 1st class passengers had 96%+ survival rate\")\n",
    "print(\"âœ“ Male 3rd class passengers had lowest survival (~13%)\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e953d66",
   "metadata": {},
   "source": [
    "### ðŸŽ¯ Benefits of Advanced Feature Engineering\n",
    "\n",
    "**Why These Features Matter:**\n",
    "\n",
    "1. **Better Signal Extraction**:\n",
    "   - `title_grouped` reduces noise from 17+ rare titles â†’ 5 meaningful categories\n",
    "   - `sex_pclass` captures the \"women & children first\" policy better than sex/class alone\n",
    "   - `fare_per_person` accounts for group tickets vs individual fares\n",
    "\n",
    "2. **Handling Missing Data Intelligently**:\n",
    "   - `age_missing` flag preserves information about missingness patterns\n",
    "   - `fare_inference` fills missing fares with class-appropriate estimates\n",
    "   - `cabin_known` treats cabin availability as a feature (not just missing data)\n",
    "\n",
    "3. **Domain Knowledge Integration**:\n",
    "   - Family size features reflect historical accounts of families staying together\n",
    "   - Title grouping aligns with 1912 social hierarchy\n",
    "   - Cabin deck extraction uses ship layout (upper decks = better access to lifeboats)\n",
    "\n",
    "4. **Production-Ready**:\n",
    "   - All features implemented in `src/features/build_features.py`\n",
    "   - Used by the Flask API (`/predict` endpoint)\n",
    "   - Fully tested in `test_api.py` (100% pass rate)\n",
    "\n",
    "**Next Steps:**\n",
    "- These advanced features will be automatically applied during model training\n",
    "- The production API uses `include_advanced=True` by default\n",
    "- You can toggle advanced features on/off for A/B testing performance gains\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41dd50b1-35df-4bea-bf2e-d2b3c99c4567",
   "metadata": {},
   "source": [
    "### 3.2 Train/Test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39dee43b-d2be-48bd-a7cf-b48c0288d26c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I want to make sure that the data remains as a pandas dataframe when splitting. Features get split based on the train_test flag from earlier:\n",
    "X_train = pd.DataFrame(all_dummies[all_dummies.train_test == 1].drop(['train_test'], axis=1))\n",
    "X_test = pd.DataFrame(all_dummies[all_dummies.train_test == 0].drop(['train_test'], axis=1))\n",
    "\n",
    "# Same logic for training labels\n",
    "y_train = all_data[all_data.train_test == 1].Survived.values\n",
    "print('y_train shape ->', y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b555fb-02cb-4c69-aa99-61b924464bab",
   "metadata": {},
   "source": [
    "### 3.3 Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e70414-48f5-4909-8a9f-2973531119f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize BEFORE scaling - show the scale differences\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "features_to_scale = ['Age', 'SibSp', 'Parch', 'norm_fare']\n",
    "\n",
    "for idx, feature in enumerate(features_to_scale):\n",
    "    ax = axes[idx // 2, idx % 2]\n",
    "    all_dummies[feature].hist(bins=30, ax=ax, color='lightcoral', edgecolor='black', alpha=0.7)\n",
    "    ax.set_title(f'BEFORE Scaling: {feature}', fontsize=12, fontweight='bold')\n",
    "    ax.set_xlabel(f'{feature} Value', fontsize=10)\n",
    "    ax.set_ylabel('Frequency', fontsize=10)\n",
    "    ax.axvline(all_dummies[feature].mean(), color='red', linestyle='--', linewidth=2, label=f'Mean: {all_dummies[feature].mean():.2f}')\n",
    "    ax.axvline(all_dummies[feature].std(), color='blue', linestyle=':', linewidth=2, label=f'Std: {all_dummies[feature].std():.2f}')\n",
    "    ax.legend(fontsize=8)\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.suptitle('Feature Distributions BEFORE StandardScaler', fontsize=16, fontweight='bold', y=1.00)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Apply StandardScaler\n",
    "scale = StandardScaler()\n",
    "all_dummies_scaled = all_dummies.copy()\n",
    "all_dummies_scaled[features_to_scale] = scale.fit_transform(all_dummies_scaled[features_to_scale])\n",
    "\n",
    "# Visualize AFTER scaling - all features now have mean=0, std=1\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "for idx, feature in enumerate(features_to_scale):\n",
    "    ax = axes[idx // 2, idx % 2]\n",
    "    all_dummies_scaled[feature].hist(bins=30, ax=ax, color='lightgreen', edgecolor='black', alpha=0.7)\n",
    "    ax.set_title(f'AFTER Scaling: {feature}', fontsize=12, fontweight='bold')\n",
    "    ax.set_xlabel('Standardized Value (z-score)', fontsize=10)\n",
    "    ax.set_ylabel('Frequency', fontsize=10)\n",
    "    ax.axvline(all_dummies_scaled[feature].mean(), color='red', linestyle='--', linewidth=2, label=f'Mean: {all_dummies_scaled[feature].mean():.4f}')\n",
    "    ax.axvline(all_dummies_scaled[feature].std(), color='blue', linestyle=':', linewidth=2, label=f'Std: {all_dummies_scaled[feature].std():.4f}')\n",
    "    ax.legend(fontsize=8)\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "    ax.set_xlim([-4, 4])  # Standardized features typically in this range\n",
    "\n",
    "plt.suptitle('Feature Distributions AFTER StandardScaler (mean=0, std=1)', fontsize=16, fontweight='bold', y=1.00)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸ“Š Scaling Transformation Summary:\")\n",
    "print(f\"   âœ… All features now have mean â‰ˆ 0 and standard deviation â‰ˆ 1\")\n",
    "print(f\"   âœ… Features are now on the same scale - prevents dominance by large-magnitude features\")\n",
    "print(f\"   âœ… Critical for distance-based algorithms (KNN, SVC) and regularized models (Logistic, Ridge)\")\n",
    "\n",
    "# Split into train/test\n",
    "X_train_scaled = all_dummies_scaled[all_dummies_scaled.train_test == 1].drop(['train_test'], axis=1)\n",
    "X_test_scaled = all_dummies_scaled[all_dummies_scaled.train_test == 0].drop(['train_test'], axis=1)\n",
    "y_train = all_data[all_data.train_test == 1].Survived\n",
    "\n",
    "print(f\"\\nðŸŽ¯ Final Dataset Shapes:\")\n",
    "print(f\"   X_train_scaled: {X_train_scaled.shape}\")\n",
    "print(f\"   X_test_scaled: {X_test_scaled.shape}\")\n",
    "print(f\"   y_train: {y_train.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d078c99-6e4a-4def-b83b-6ab283a8b75d",
   "metadata": {},
   "source": [
    "### 3.4 Finalized Data Correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c09df486-643a-4cc0-baa8-f49a8d7ee1d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate into a full dataset\n",
    "full_df = pd.concat([X_train, y_train], axis=1)\n",
    "\n",
    "correlation = full_df.corr()['Survived'].sort_values(ascending=False)\n",
    "\n",
    "# Correlation graph\n",
    "correlation[1:].plot(kind='bar', figsize=(10,5), title='Survivability Correlation')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c1685a-9c02-4d07-96d2-6b1f52deb863",
   "metadata": {},
   "source": [
    "## 4. Metrics and Evaluation Strategy\n",
    "\n",
    "**Competition Metric**: **Accuracy** (required by Kaggle leaderboard)\n",
    "\n",
    "**Why also track F1 Score?**\n",
    "- Dataset is imbalanced (~38% survival rate)\n",
    "- Accuracy can be misleading with imbalanced classes\n",
    "- F1 Score balances precision and recall, giving a more complete picture\n",
    "\n",
    "**Cross-Validation Strategy**:\n",
    "- **Stratified 5-fold CV**: Maintains class distribution across folds\n",
    "- Essential for imbalanced datasets (62% died, 38% survived)\n",
    "- Reduces overfitting risk\n",
    "- More reliable than single train/test split\n",
    "\n",
    "**What We Measure:**\n",
    "- **Training Performance**: How well model learns from data\n",
    "- **Validation Performance**: How well model generalizes\n",
    "- **Feature Importance**: Which features drive predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1464fe89",
   "metadata": {},
   "source": [
    "### 4.0 Handling Class Imbalance\n",
    "\n",
    "The Titanic dataset is **imbalanced** with a ~62/38 split (died/survived). To handle this:\n",
    "\n",
    "**Stratified K-Fold Cross-Validation:**\n",
    "- Ensures each fold maintains the same class distribution as the full dataset\n",
    "- Prevents bias toward the majority class\n",
    "- Provides more reliable performance estimates\n",
    "\n",
    "**Weighted Metrics:**\n",
    "- F1-weighted score accounts for class imbalance\n",
    "- Gives appropriate weight to minority class performance\n",
    "\n",
    "**Why This Matters:**\n",
    "- Regular K-Fold might create folds with very different survival rates\n",
    "- Could lead to unreliable model evaluation\n",
    "- Stratification ensures consistent, reproducible results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9650c6fd",
   "metadata": {},
   "source": [
    "**Additional Class Imbalance Techniques (Not Applied):**\n",
    "\n",
    "While we use Stratified CV and weighted metrics, other techniques exist:\n",
    "\n",
    "1. **SMOTE (Synthetic Minority Over-sampling)**\n",
    "   - Creates synthetic samples of minority class\n",
    "   - Risk: May introduce noise or overfitting\n",
    "   - Not used: Our dataset already has sufficient samples (891 total)\n",
    "\n",
    "2. **Class Weights in Models**\n",
    "   - Many sklearn models support `class_weight='balanced'`\n",
    "   - Automatically adjusts loss function\n",
    "   - Not used: Stratified CV + weighted F1 already handles imbalance well\n",
    "\n",
    "3. **Threshold Adjustment**\n",
    "   - Tune decision threshold (default: 0.5)\n",
    "   - Optimize for specific metric (precision vs recall)\n",
    "   - Not used: Kaggle evaluates on accuracy, not threshold-dependent\n",
    "\n",
    "4. **Ensemble with Undersampling**\n",
    "   - Create multiple balanced subsets\n",
    "   - Train models on each, then ensemble\n",
    "   - Not used: Would lose training data and not significantly improve results\n",
    "\n",
    "**Our Approach:**\n",
    "Stratified CV + Weighted Metrics provides excellent balance without data manipulation risks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f73ce0bd-bf38-4788-9dd2-a2782e0c44c1",
   "metadata": {},
   "source": [
    "### 4.1 Model Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc37dcc3-6a1f-4421-888a-5a63e6ce5323",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up a constant for the metric input to easily shift between accuracy and F1 score\n",
    "METRIC = 'accuracy'\n",
    "# METRIC = 'f1_weighted'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1860a363-ff0f-4158-a131-3dea537fb719",
   "metadata": {},
   "source": [
    "### 4.2 Metric Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b9b5ef-a1d9-4ea1-bb23-5dcc55a8f731",
   "metadata": {},
   "source": [
    "### 4.3 Helper Functions\n",
    "This section is also where I have stored all functions with miscellaneous uses that will be used throughout the rest of this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3098fd0-4f62-4a60-a580-9c0e0dbfb9f1",
   "metadata": {},
   "source": [
    "## 5. Model Selection and Baseline Evaluation\n",
    "To start with I would like to see how various different models perform with default parameters. I tried the models in the below list using 5 fold cross validation to get a baseline. I can later compare these results to the tuned version of these models and see how much tuning improves the performance of each. I can get away with using large number of models for this project since the dataset is not large at all and fitting all these models should only take a few seconds each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "552e75f1-f8b7-4ac5-81aa-ee4ad39e01a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(max_iter=2000),\n",
    "    'Decision Tree Classifier': DecisionTreeClassifier(random_state=1),\n",
    "    'Random Forest Classifier': RandomForestClassifier(random_state=1),\n",
    "    'GaussianNB': GaussianNB(),\n",
    "    'KNeighbors Classifier': KNeighborsClassifier(),\n",
    "    'SVC': SVC(probability=True),\n",
    "    'XGB Classifier': XGBClassifier(random_state=1),\n",
    "    'CatBoost Classifier': CatBoostClassifier(silent=True)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0626e9fc-109f-48fa-904b-22f581308d2b",
   "metadata": {},
   "source": [
    "### 5.1 Scaled vs non-scaled data testing\n",
    "I want to check what difference scaling the data will have on the scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c35295-4ff0-485a-b6fe-b70f8a30d2c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate models on non-scaled data\n",
    "_ = evaluate_models_cv(models, X_train, y_train, cv_folds=config.CV_FOLDS, scoring='accuracy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "644469f8-7955-4081-ad3c-089472f9038d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate models on scaled data  \n",
    "_ = evaluate_models_cv(models, X_train_scaled, y_train, cv_folds=config.CV_FOLDS, scoring='accuracy')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "993ee7c4-bc6e-4122-955a-6f3397ee5154",
   "metadata": {},
   "source": [
    "Some models are unaffected by the scaling and the rest got improved scores. GaussianNB is the only exception since it's score reduced. That model's overall performance is significantly lower than the rest so it will be getting dropped from the list."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a29dec7d-f9b4-40e7-bbcf-20da155c4fab",
   "metadata": {},
   "source": [
    "### 5.2 Voting classifier \n",
    "\n",
    "The VotingClassifier in scikit-learn combines the predictions from multiple individual classifiers to make a final prediction. There are two main types of voting strategies:\n",
    "\n",
    "- Hard Voting Classifier - In the hard voting scheme, each classifier in the ensemble casts a single \"vote\" for a particular class (\"yes\" or \"no\"). The final prediction is determined by a majority vote. To ensure a decisive outcome, it's recommended to use an odd number of classifiers.\n",
    "\n",
    "- Soft Voting Classifier - The soft voting scheme considers the confidence or probability assigned by each classifier for each class. Instead of a simple majority vote, it takes the average of the predicted probabilities. If the average confidence for a particular class surpasses 50%, that class is chosen as the final prediction.\n",
    "\n",
    "The choice between hard and soft voting often depends on the characteristics of the individual classifiers in the ensemble. Hard voting is suitable when classifiers are diverse and have varying strengths, while soft voting is effective when classifiers provide probability estimates, allowing for a more nuanced decision-making process.\n",
    "\n",
    "The model will require tuples of model name and model object which I can generate using the models dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39822e0f-fca4-43cc-bb1f-d2a3a18609cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build model\n",
    "voting_clf_model_tuples = [(model_name, model) for model_name, model in models.items()]\n",
    "voting_clf = VotingClassifier(estimators=voting_clf_model_tuples, voting='soft')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e253488c-33bd-43b5-ae1b-6897a4508e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Voting Classifier\n",
    "_ = evaluate_models_cv(\n",
    "    {'Voting Classifier': voting_clf}, \n",
    "    X_train_scaled.values, \n",
    "    y_train,\n",
    "    cv_folds=config.CV_FOLDS,\n",
    "    scoring='accuracy'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ef5a8d-dd3e-4482-9038-c89fcabb77bd",
   "metadata": {},
   "source": [
    "### 5.3 Voting Classifier Predictions\n",
    "This model is performing better than the rest so let's generate a submissions file to use as a baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f7a488-35f9-4f3a-b004-602b119a3c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate baseline voting classifier submission\n",
    "voting_clf.fit(X_train_scaled.values, y_train)\n",
    "voting_clf_preds = voting_clf.predict(X_test_scaled.values).astype(int)\n",
    "\n",
    "# Generate submission file\n",
    "generate_kaggle_submission(\n",
    "    predictions=voting_clf_preds,\n",
    "    passenger_ids=test.PassengerId,\n",
    "    file_name='01_voting_clf_submission.csv',\n",
    "    output_dir='submissions'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8362a4ce-3fce-43c5-b9ed-8fee0ff48e36",
   "metadata": {},
   "source": [
    "Let's set up a function to use for generating submission files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d86520b-32bc-4344-834c-9aa79f476103",
   "metadata": {},
   "source": [
    "### 5.4 Model List Cleanup\n",
    "Let's eliminate some of the models that are not performing that well:\n",
    "- GaussianNB - poor performance\n",
    "- Decision Tree Classifier - poor performance and forest should be better in most situations\n",
    "- SVC - this model has been performing well but it's causing trouble when tuning hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d877701d-3b6d-4493-bbc8-4a6137a399ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_to_remove = ['GaussianNB', 'Decision Tree Classifier']\n",
    "for model in models_to_remove:\n",
    "    if model in models:\n",
    "        del models[model]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b22b04e-cb45-4714-8260-907bf9939371",
   "metadata": {},
   "source": [
    "## 6. Hyperparameter Tuning\n",
    "\n",
    "After establishing baselines, we'll systematically optimize each model using **GridSearchCV** with **Stratified K-Fold Cross-Validation**.\n",
    "\n",
    "**Tuning Strategy:**\n",
    "- **Grid Search**: Exhaustive search over specified parameter values\n",
    "- **Stratified CV**: Maintains class distribution across folds (essential for imbalanced data)\n",
    "- **Weighted F1 Score**: Optimization metric that accounts for class imbalance\n",
    "- **Reduced Grids**: Some grids simplified after initial exploration to balance performance vs. compute time\n",
    "\n",
    "**Models to Tune:**\n",
    "- Logistic Regression\n",
    "- Random Forest Classifier\n",
    "- K-Neighbors Classifier\n",
    "- Support Vector Classifier (SVC)\n",
    "- XGBoost Classifier\n",
    "- CatBoost Classifier\n",
    "\n",
    "The automated hyperparameter search will report best parameters and scores for each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b1fd2ac-37fe-4b0c-81f7-88a46773a5e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_params = {\n",
    "    'Logistic Regression': {\n",
    "        'max_iter' : [10, 50, 100, 1000],\n",
    "        'penalty' : ['l1', 'l2'],\n",
    "        'C' : np.logspace(-4, 4, 20),\n",
    "        'solver' : ['liblinear']\n",
    "        },\n",
    "    # 'Random Forest Classifier': {\n",
    "    #     'n_estimators': [100, 500, 1000], \n",
    "    #     'bootstrap': [True, False],\n",
    "    #     'max_depth': [3, 5, 10, 20, 50, 75, 100, None],\n",
    "    #     'max_features': ['auto', 'sqrt', 'log2'],\n",
    "    #     'min_samples_leaf': [1, 2, 4, 10],\n",
    "    #     'min_samples_split': [2, 5, 10]\n",
    "    #     },\n",
    "    'Random Forest Classifier': {\n",
    "        'n_estimators': [100, 300, 450],\n",
    "        'criterion':['gini', 'entropy'], \n",
    "        'bootstrap': [True],\n",
    "        'max_depth': [10, 15],\n",
    "        'max_features': ['sqrt', 20, 40],\n",
    "        'min_samples_leaf': [1, 2],\n",
    "        'min_samples_split': [.5, 2]\n",
    "        },\n",
    "    'KNeighbors Classifier': {\n",
    "        'n_neighbors' : [3, 5, 7, 9, 12, 15],\n",
    "        'weights' : ['uniform', 'distance'],\n",
    "        'algorithm' : ['ball_tree', 'kd_tree'],\n",
    "        'p' : [1, 2]\n",
    "        },\n",
    "    'SVC': [\n",
    "        {'kernel': ['rbf'], 'gamma': [.1, .5, 1, 2, 5, 10], 'C': [.1, 1, 10]},\n",
    "        {'kernel': ['linear'], 'C': [.1, 1, 10]},\n",
    "        {'kernel': ['poly', 'linear', 'rbf'], 'degree' : [1, 2, 3], 'C': [.1, 1, 10]}\n",
    "        ],\n",
    "    'XGB Classifier': {\n",
    "        'n_estimators': [500, 550],\n",
    "        'colsample_bytree': [.5, .6, .75],\n",
    "        'max_depth': [10, None],\n",
    "        'reg_alpha': [1],\n",
    "        'reg_lambda': [5, 10, 15],\n",
    "        'subsample': [.55, .6, .65],\n",
    "        'learning_rate':[.5],\n",
    "        'gamma':[.25, .5, 1],\n",
    "        'min_child_weight':[0.01],\n",
    "        'sampling_method': ['uniform']\n",
    "        },\n",
    "    'CatBoost Classifier': {\n",
    "        'iterations': [400, 500],\n",
    "        'learning_rate': [0.01, 0.05, 0.1],\n",
    "        'depth': [2, 4, 6],\n",
    "        'l2_leaf_reg': [1, 2],\n",
    "        'border_count': [64, 128]\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "329fa729-abc8-49b1-bdeb-89d421847915",
   "metadata": {},
   "source": [
    "Let's now run the hyperparameter grid search and save the results for each model along with the model's best performing version in a dictionary which will allow for easy access to everything I might need in regards to these models later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1abc99be-bbcd-4e39-af2b-add9ec29367b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimize models using GridSearchCV\n",
    "tuned_models = optimize_models(models, model_params, X_train_scaled, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d28a7e71",
   "metadata": {},
   "source": [
    "### 6.1 Detailed Cross-Validation Analysis\n",
    "\n",
    "Let's analyze the stability and consistency of our tuned models across CV folds.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55cf6b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get detailed CV scores for tuned models\n",
    "print(\"=\"*70)\n",
    "print(\"CROSS-VALIDATION SCORE ANALYSIS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "cv_scores_dict = {}\n",
    "for model_name, (model, mean_score) in tuned_models.items():\n",
    "    scores_detail = get_cv_scores_detailed(\n",
    "        model, \n",
    "        X_train_scaled, \n",
    "        y_train,\n",
    "        cv_folds=config.CV_FOLDS,\n",
    "        scoring='accuracy'\n",
    "    )\n",
    "    cv_scores_dict[model_name] = scores_detail['scores']\n",
    "    \n",
    "    print(f\"\\n{model_name}:\")\n",
    "    print(f\"  Mean:   {scores_detail['mean']:.4f}\")\n",
    "    print(f\"  Std:    {scores_detail['std']:.4f}\")\n",
    "    print(f\"  Range:  [{scores_detail['min']:.4f}, {scores_detail['max']:.4f}]\")\n",
    "    print(f\"  Folds:  {[f'{s:.4f}' for s in scores_detail['scores']]}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "\n",
    "# Visualize CV score distributions\n",
    "fig = plot_cv_score_distribution(\n",
    "    cv_scores_dict,\n",
    "    title='Cross-Validation Score Distributions (5-Fold)',\n",
    "    figsize=(14, 6)\n",
    ")\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"INTERPRETATION:\")\n",
    "print(\"=\"*70)\n",
    "print(\"â€¢ Lower variance = More stable/reliable model\")\n",
    "print(\"â€¢ Higher median = Better average performance\")\n",
    "print(\"â€¢ Look for models with high median AND low variance\")\n",
    "print(\"=\"*70)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "943453a6-c2fa-4084-b9e2-a8f9c5bd9a7a",
   "metadata": {},
   "source": [
    "Unsurprisingle, age and sex are big determinants of the outcome. It's quite interesting that the normalized Fare is the feature with the greatest importance.\n",
    "\n",
    "### 6.1 Tuned XGB Predictions\n",
    "XGB Classifier is the best performing model so far so let's generate a submissions file using this tuned version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55cc8a26-f542-4d6f-b988-570ee17a8305",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate XGBoost tuned predictions\n",
    "xgb_predictions = tuned_models['XGB Classifier'][0].predict(X_test_scaled).astype(int)\n",
    "generate_kaggle_submission(xgb_predictions, test.PassengerId, '02_xgb_tuned_submission.csv', 'submissions')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e617396-705f-48fb-8021-f5372672eec4",
   "metadata": {},
   "source": [
    "### 6.2 Feature Importances\n",
    "I am quite interested in the feature importances so let's have a look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "015b73b4-914e-4bfe-be8b-ddbf2c2a4b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_xgb = tuned_models['XGB Classifier'][0].fit(X_train_scaled, y_train)\n",
    "feat_importances = pd.Series(best_xgb.feature_importances_, index=X_train_scaled.columns)\n",
    "feat_importances.nlargest(20).plot(kind='barh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd33125-7649-493d-802e-b1e899f52a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_rf = tuned_models['Random Forest Classifier'][0].fit(X_train_scaled, y_train)\n",
    "feat_importances = pd.Series(best_rf.feature_importances_, index=X_train_scaled.columns)\n",
    "feat_importances.nlargest(20).plot(kind='barh')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea419ead",
   "metadata": {},
   "source": [
    "### 6.3 Comprehensive Feature Importance Analysis\n",
    "\n",
    "Let's compare feature importances across multiple models to understand which features consistently drive predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca235195",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare feature importances across tree-based models\n",
    "tree_models = {\n",
    "    'Random Forest': tuned_models['Random Forest Classifier'][0],\n",
    "    'XGBoost': tuned_models['XGB Classifier'][0],\n",
    "    'CatBoost': tuned_models['CatBoost Classifier'][0]\n",
    "}\n",
    "\n",
    "# Create subplots for each model\n",
    "fig, axes = plt.subplots(1, 3, figsize=(20, 6))\n",
    "\n",
    "for (name, model), ax in zip(tree_models.items(), axes):\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    feat_imp = pd.Series(model.feature_importances_, index=X_train_scaled.columns)\n",
    "    feat_imp.nlargest(15).sort_values().plot(kind='barh', ax=ax, color='skyblue')\n",
    "    ax.set_title(f'{name} - Top 15 Features', fontsize=14, fontweight='bold')\n",
    "    ax.set_xlabel('Importance', fontsize=12)\n",
    "    ax.grid(axis='x', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Aggregate feature importance across models\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"AGGREGATED FEATURE IMPORTANCE (Average Across Models)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "all_importances = pd.DataFrame()\n",
    "for name, model in tree_models.items():\n",
    "    all_importances[name] = model.feature_importances_\n",
    "\n",
    "all_importances.index = X_train_scaled.columns\n",
    "all_importances['Mean'] = all_importances.mean(axis=1)\n",
    "all_importances['Std'] = all_importances.std(axis=1)\n",
    "\n",
    "top_features = all_importances.sort_values('Mean', ascending=False).head(10)\n",
    "print(\"\\nTop 10 Most Important Features:\")\n",
    "print(top_features[['Mean', 'Std']].to_string())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96cc271e-9bb1-4164-9d0a-d4f86466fdbf",
   "metadata": {},
   "source": [
    "## 7. Additional Ensemble Approaches using Voting Classifier\n",
    "1) Experimented with a <b>Hard Voting</b> classifier\n",
    "\n",
    "2) Experimented with a <b>Soft Voting</b> classifier\n",
    "\n",
    "3) Experimented with <b>Soft Voting while eliminating worst performing model</b>\n",
    "\n",
    "4) Experimented with <b>Soft Voting while eliminating the two worst performing</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d6ac53-3449-4bdd-9309-84808b437cc5",
   "metadata": {},
   "source": [
    "Let's run a voting classifier using the tuned models. I also want to try and eliminate some of the models to see if that improves performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43017871-345f-465a-8ae3-6585f2d7053f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test hard voting with tuned models\n",
    "tuned_model_tuples = [(model_name, model[0]) for model_name, model in tuned_models.items()]\n",
    "voting_clf_all = VotingClassifier(estimators=tuned_model_tuples, voting='hard')\n",
    "_ = evaluate_models_cv({'Voting Classifier': voting_clf_all}, X_train_scaled, y_train, config.CV_FOLDS, 'accuracy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b76f83eb-99a1-42f4-82f4-ac876cb44f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test soft voting with tuned models\n",
    "tuned_model_tuples = [(model_name, model[0]) for model_name, model in tuned_models.items()]\n",
    "voting_clf_all = VotingClassifier(estimators=tuned_model_tuples, voting='soft')\n",
    "_ = evaluate_models_cv({'Voting Classifier': voting_clf_all}, X_train_scaled, y_train, config.CV_FOLDS, 'accuracy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77cfedee-7c60-4a1e-87d2-2ae5ea55ec57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test soft voting without worst performer (Logistic Regression)\n",
    "tuned_model_tuples = [(model_name, model[0]) for model_name, model in tuned_models.items() if model_name not in ['Logistic Regression']]\n",
    "voting_clf_all = VotingClassifier(estimators=tuned_model_tuples, voting='soft')\n",
    "_ = evaluate_models_cv({'Voting Classifier': voting_clf_all}, X_train_scaled, y_train, config.CV_FOLDS, 'accuracy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e733fdda-045d-4eb4-b4bb-83097758865c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test soft voting without two worst performers\n",
    "tuned_model_tuples = [(model_name, model[0]) for model_name, model in tuned_models.items() if model_name not in ['Logistic Regression', 'KNeighbors Classifier']]\n",
    "voting_clf_all = VotingClassifier(estimators=tuned_model_tuples, voting='soft')\n",
    "_ = evaluate_models_cv({'Voting Classifier': voting_clf_all}, X_train_scaled, y_train, config.CV_FOLDS, 'accuracy')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72919aad-7d0a-466f-9297-8210c490f9fc",
   "metadata": {},
   "source": [
    "Hard voting seems to be the way to go. Also, removing the worst performing models does produce an improved score as well so let's keep that approach.\n",
    "\n",
    "### 7.1 Voting Classifier Optimisation\n",
    "In a soft voting classifier you can apply weights to each of the models. Let's use a grid search to explore different weightings. \n",
    "\n",
    "To not have to adjust the grid every time the models count change we generate a grid using itertools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c46259b-38bc-4d80-8f40-1c202bccaccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimize voting classifier weights\n",
    "combinations = itertools.product([1, 2], repeat=len(tuned_model_tuples))\n",
    "combinations = [list(comb) for comb in combinations if len(set(comb)) != 1]\n",
    "\n",
    "voting_classifier_params = {'Voting Classifier': {'weights': combinations, 'voting': ['soft', 'hard']}}\n",
    "\n",
    "# Tune the voting classifier\n",
    "tuned_voting_clf = optimize_models(\n",
    "    {'Voting Classifier': voting_clf_all},\n",
    "    voting_classifier_params,\n",
    "    X_train_scaled,\n",
    "    y_train\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ffb4a7b-4002-48e4-917e-c394ce116407",
   "metadata": {},
   "source": [
    "The performence does improve after tuning but not by that much. Let's add this tuned Voting Classifier to the dictionary of models which can then be sorted which will allow the best performing model to be easily retrieved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce5d7a7-c0b6-4fb9-9105-4abf04a92803",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding the Voting Classifier to the tuned models dictionary and sorting it to easily retrieve the best performing model\n",
    "tuned_models = {**tuned_voting_clf, **tuned_models}\n",
    "sorted_tuned_models = dict(sorted(tuned_models.items(), key=lambda item: -item[1][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d88cc9-1be7-465f-932d-a9cafd7fc4eb",
   "metadata": {},
   "source": [
    "## 8. Final Submissions\n",
    "Let's take the best performing model from our tuned_models dict and use it to generate the final submission file. I'll also save that model for later use."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56ca8369",
   "metadata": {},
   "source": [
    "### 7.2 Model Performance Visualization\n",
    "\n",
    "Let's create comprehensive visualizations comparing all tuned models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d6087b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract scores from tuned models for comparison\n",
    "model_scores = {name: score for name, (model, score) in tuned_models.items()}\n",
    "\n",
    "# Create comparison visualization\n",
    "fig = plot_model_comparison(\n",
    "    model_scores,\n",
    "    title='Tuned Model Performance Comparison (Cross-Validation)',\n",
    "    metric_name=f'{METRIC.title()} Score',\n",
    "    figsize=(12, 6)\n",
    ")\n",
    "plt.show()\n",
    "\n",
    "# Print top 3 performers\n",
    "print(\"\\nðŸ† Top 3 Performing Models:\")\n",
    "for i, (name, score) in enumerate(sorted(model_scores.items(), key=lambda x: -x[1])[:3], 1):\n",
    "    print(f\"{i}. {name}: {score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b251611a-d7c3-41a2-9a52-f328ac7f417d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taking best model from dictionary\n",
    "best_model_name, best_model_tuple = list(sorted_tuned_models.items())[0]\n",
    "best_model, best_model_score = best_model_tuple\n",
    "print(f\"Best performing model -> {best_model_name} with anm F1 score of {round(best_model_score * 100, 1)}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba3ff58",
   "metadata": {},
   "source": [
    "### 8.1 Comprehensive Model Evaluation\n",
    "\n",
    "Before finalizing our submission, let's perform a detailed evaluation with confusion matrix and ROC curve analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad258882",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions on training set for evaluation\n",
    "y_train_pred = best_model.predict(X_train_scaled)\n",
    "y_train_pred_proba = best_model.predict_proba(X_train_scaled)[:, 1] if hasattr(best_model, 'predict_proba') else None\n",
    "\n",
    "# Create confusion matrix\n",
    "fig = plot_confusion_matrix(\n",
    "    y_train,\n",
    "    y_train_pred,\n",
    "    title=f'Confusion Matrix - {best_model_name} (Training Set)',\n",
    "    figsize=(8, 6)\n",
    ")\n",
    "plt.show()\n",
    "\n",
    "# Plot ROC curve if probability predictions available\n",
    "if y_train_pred_proba is not None:\n",
    "    fig = plot_roc_curve(\n",
    "        y_train,\n",
    "        y_train_pred_proba,\n",
    "        title=f'ROC Curve - {best_model_name}',\n",
    "        figsize=(8, 6)\n",
    "    )\n",
    "    plt.show()\n",
    "\n",
    "# Print detailed classification metrics\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(f\"DETAILED METRICS - {best_model_name}\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nAccuracy:  {accuracy_score(y_train, y_train_pred):.4f}\")\n",
    "print(f\"F1 Score:  {f1_score(y_train, y_train_pred, average='weighted'):.4f}\")\n",
    "if y_train_pred_proba is not None:\n",
    "    print(f\"ROC-AUC:   {roc_auc_score(y_train, y_train_pred_proba):.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*60)\n",
    "print(\"Classification Report:\")\n",
    "print(\"-\"*60)\n",
    "print(classification_report(y_train, y_train_pred, target_names=['Did Not Survive', 'Survived']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58165d76-d185-4dc8-95a8-9588bf099aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate final submission with best model\n",
    "best_model_preds = best_model.predict(X_test_scaled).astype(int)\n",
    "generate_kaggle_submission(\n",
    "    best_model_preds, \n",
    "    test.PassengerId, \n",
    "    config.SUBMISSION_OPTIMIZED,\n",
    "    config.SUBMISSIONS_DIR\n",
    ")\n",
    "\n",
    "# Save best model for future use\n",
    "save_model_pickle(best_model, 'titanic_survival_classifier.pkl', config.MODELS_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ec9c622",
   "metadata": {},
   "source": [
    "### 8.2 Final Summary\n",
    "\n",
    "**Project Results Summary:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4243eb14",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TITANIC SURVIVAL PREDICTION - PROJECT SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\nðŸŽ¯ Best Model: {best_model_name}\")\n",
    "print(f\"ðŸ“Š Cross-Validation Score: {best_model_score:.4f}\")\n",
    "print(f\"ðŸ”¢ Random Seed: {config.RANDOM_SEED} (for reproducibility)\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*70)\n",
    "print(\"KEY FINDINGS:\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "# Get top 5 important features if available\n",
    "if hasattr(best_model, 'feature_importances_'):\n",
    "    top_5_features = pd.Series(\n",
    "        best_model.feature_importances_,\n",
    "        index=X_train_scaled.columns\n",
    "    ).nlargest(5)\n",
    "    \n",
    "    print(\"\\nTop 5 Most Important Features:\")\n",
    "    for i, (feat, imp) in enumerate(top_5_features.items(), 1):\n",
    "        print(f\"  {i}. {feat}: {imp:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*70)\n",
    "print(\"FILES GENERATED:\")\n",
    "print(\"-\"*70)\n",
    "print(f\"  âœ“ {config.SUBMISSION_VOTING_CLF} (Baseline ensemble)\")\n",
    "print(f\"  âœ“ {config.SUBMISSION_XGB_TUNED} (Best individual model)\")\n",
    "print(f\"  âœ“ {config.SUBMISSION_OPTIMIZED} (Final submission)\")\n",
    "print(\"  âœ“ titanic_survival_classifier.pkl (Saved model)\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*70)\n",
    "print(\"METHODOLOGY HIGHLIGHTS:\")\n",
    "print(\"-\"*70)\n",
    "print(\"  â€¢ Comprehensive EDA with visualization\")\n",
    "print(\"  â€¢ Feature engineering (cabin_multiple, name_title, norm_fare)\")\n",
    "print(\"  â€¢ 8 different classifiers compared\")\n",
    "print(\"  â€¢ GridSearchCV hyperparameter tuning\")\n",
    "print(\"  â€¢ Voting ensemble methods\")\n",
    "print(\"  â€¢ Cross-validation for robust evaluation\")\n",
    "print(\"  â€¢ SHAP analysis for model interpretability\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*70)\n",
    "print(\"READY FOR KAGGLE SUBMISSION:\")\n",
    "print(\"-\"*70)\n",
    "print(f\"  ðŸ“ Location: {config.SUBMISSIONS_DIR}/\")\n",
    "print(f\"  ðŸ“„ Submit: {config.SUBMISSION_OPTIMIZED}\")\n",
    "print(\"=\"*70 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e809c5",
   "metadata": {},
   "source": [
    "## 9. Model Explainability with SHAP\n",
    "\n",
    "**SHAP (SHapley Additive exPlanations)** provides model-agnostic explanations by computing the contribution of each feature to individual predictions.\n",
    "\n",
    "SHAP values help us understand:\n",
    "- **Global Importance**: Which features matter most across all predictions\n",
    "- **Local Explanations**: Why a specific prediction was made\n",
    "- **Feature Interactions**: How features work together\n",
    "- **Decision Boundaries**: Where the model changes its predictions\n",
    "\n",
    "This analysis uses SHAP to explain our best-performing model's predictions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75e4df76",
   "metadata": {},
   "source": [
    "### 9.1 Initialize SHAP Explainer\n",
    "\n",
    "For tree-based models (XGBoost, Random Forest, CatBoost), we use TreeExplainer which is optimized for speed and accuracy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e13d29dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create SHAP explainer for the best model\n",
    "print(f\"Creating SHAP explainer for {best_model_name}...\")\n",
    "\n",
    "if hasattr(best_model, 'estimators_'):\n",
    "    # For ensemble models (VotingClassifier), extract the underlying model\n",
    "    print(\"Note: Using first estimator from ensemble for SHAP analysis\")\n",
    "    model_for_shap = best_model.estimators_[0]\n",
    "else:\n",
    "    model_for_shap = best_model\n",
    "\n",
    "# Use TreeExplainer for tree-based models\n",
    "try:\n",
    "    explainer = shap.TreeExplainer(model_for_shap)\n",
    "    print(\"âœ“ TreeExplainer initialized successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"TreeExplainer not supported, using KernelExplainer: {e}\")\n",
    "    # Fallback to KernelExplainer (slower but model-agnostic)\n",
    "    # Use a sample of training data as background\n",
    "    explainer = shap.KernelExplainer(\n",
    "        model_for_shap.predict_proba if hasattr(model_for_shap, 'predict_proba') else model_for_shap.predict,\n",
    "        shap.sample(X_train_scaled, 100)\n",
    "    )\n",
    "\n",
    "# Calculate SHAP values for training set (using subset for speed)\n",
    "print(\"Calculating SHAP values (this may take a moment)...\")\n",
    "sample_size = min(500, len(X_train_scaled))\n",
    "X_sample = X_train_scaled.sample(n=sample_size, random_state=config.RANDOM_SEED) if isinstance(X_train_scaled, pd.DataFrame) else X_train_scaled[:sample_size]\n",
    "\n",
    "shap_values = explainer.shap_values(X_sample)\n",
    "\n",
    "print(f\"âœ“ SHAP values calculated for {sample_size} samples\")\n",
    "print(f\"  SHAP values shape: {shap_values.shape if isinstance(shap_values, np.ndarray) else shap_values[1].shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "567c4539",
   "metadata": {},
   "source": [
    "### 9.2 SHAP Summary Plot - Global Feature Importance\n",
    "\n",
    "The summary plot shows:\n",
    "- **Distribution** of SHAP values for each feature\n",
    "- **Impact direction**: Red (high feature value) vs Blue (low feature value)\n",
    "- **Magnitude**: How much each feature affects predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a28c238",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP Summary Plot\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# For binary classification, shap_values might be a list [class0, class1]\n",
    "# We want class 1 (Survived=1) for interpretation\n",
    "shap_values_plot = shap_values[1] if isinstance(shap_values, list) else shap_values\n",
    "\n",
    "shap.summary_plot(\n",
    "    shap_values_plot, \n",
    "    X_sample,\n",
    "    feature_names=X_train_scaled.columns if isinstance(X_train_scaled, pd.DataFrame) else None,\n",
    "    show=False\n",
    ")\n",
    "plt.title('SHAP Summary Plot - Feature Impact on Survival Prediction', fontsize=14, fontweight='bold', pad=20)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"INTERPRETING THE SUMMARY PLOT:\")\n",
    "print(\"=\"*70)\n",
    "print(\"â€¢ Features are ranked by importance (top = most important)\")\n",
    "print(\"â€¢ Red points = high feature values, Blue points = low feature values\")\n",
    "print(\"â€¢ Points to the right = push prediction toward Survived=1\")\n",
    "print(\"â€¢ Points to the left = push prediction toward Survived=0\")\n",
    "print(\"=\"*70)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf56f8b",
   "metadata": {},
   "source": [
    "### 9.3 SHAP Bar Plot - Mean Absolute Feature Importance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "747cead6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP Bar Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "shap.summary_plot(\n",
    "    shap_values_plot,\n",
    "    X_sample,\n",
    "    plot_type=\"bar\",\n",
    "    feature_names=X_train_scaled.columns if isinstance(X_train_scaled, pd.DataFrame) else None,\n",
    "    show=False\n",
    ")\n",
    "plt.title('SHAP Feature Importance (Mean |SHAP value|)', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "523a01e8",
   "metadata": {},
   "source": [
    "### 9.4 Individual Prediction Explanations - Waterfall Plots\n",
    "\n",
    "Let's examine specific cases to understand model decisions:\n",
    "- **Survived Passenger**: Why did the model predict survival?\n",
    "- **Did Not Survive**: Why did the model predict death?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d461b8ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions for the sample\n",
    "sample_predictions = model_for_shap.predict(X_sample)\n",
    "\n",
    "# Find examples of survivors and non-survivors\n",
    "survived_idx = np.where(sample_predictions == 1)[0]\n",
    "died_idx = np.where(sample_predictions == 0)[0]\n",
    "\n",
    "print(f\"Found {len(survived_idx)} predicted survivors and {len(died_idx)} predicted non-survivors in sample\")\n",
    "\n",
    "# Select one example of each\n",
    "survivor_example = survived_idx[0] if len(survived_idx) > 0 else 0\n",
    "non_survivor_example = died_idx[0] if len(died_idx) > 0 else 1\n",
    "\n",
    "# Create waterfall plots\n",
    "fig, axes = plt.subplots(1, 2, figsize=(20, 6))\n",
    "\n",
    "# Waterfall plot for a survivor\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(f\"EXAMPLE 1: Predicted Survivor (Index {survivor_example})\")\n",
    "print(\"=\"*70)\n",
    "plt.sca(axes[0])\n",
    "\n",
    "# For SHAP v0.41+, use Explanation objects\n",
    "if hasattr(shap, 'Explanation'):\n",
    "    shap.waterfall_plot(\n",
    "        shap.Explanation(\n",
    "            values=shap_values_plot[survivor_example],\n",
    "            base_values=explainer.expected_value if hasattr(explainer, 'expected_value') else 0,\n",
    "            data=X_sample.iloc[survivor_example] if isinstance(X_sample, pd.DataFrame) else X_sample[survivor_example],\n",
    "            feature_names=X_train_scaled.columns if isinstance(X_train_scaled, pd.DataFrame) else None\n",
    "        ),\n",
    "        show=False\n",
    "    )\n",
    "else:\n",
    "    # Fallback for older versions\n",
    "    shap.waterfall_plot(\n",
    "        explainer.expected_value if hasattr(explainer, 'expected_value') else 0,\n",
    "        shap_values_plot[survivor_example],\n",
    "        X_sample.iloc[survivor_example] if isinstance(X_sample, pd.DataFrame) else X_sample[survivor_example],\n",
    "        feature_names=X_train_scaled.columns if isinstance(X_train_scaled, pd.DataFrame) else None\n",
    "    )\n",
    "\n",
    "axes[0].set_title(f'SHAP Waterfall: Predicted Survivor', fontsize=12, fontweight='bold')\n",
    "\n",
    "# Waterfall plot for non-survivor\n",
    "print(f\"\\nEXAMPLE 2: Predicted Non-Survivor (Index {non_survivor_example})\")\n",
    "print(\"=\"*70)\n",
    "plt.sca(axes[1])\n",
    "\n",
    "if hasattr(shap, 'Explanation'):\n",
    "    shap.waterfall_plot(\n",
    "        shap.Explanation(\n",
    "            values=shap_values_plot[non_survivor_example],\n",
    "            base_values=explainer.expected_value if hasattr(explainer, 'expected_value') else 0,\n",
    "            data=X_sample.iloc[non_survivor_example] if isinstance(X_sample, pd.DataFrame) else X_sample[non_survivor_example],\n",
    "            feature_names=X_train_scaled.columns if isinstance(X_train_scaled, pd.DataFrame) else None\n",
    "        ),\n",
    "        show=False\n",
    "    )\n",
    "else:\n",
    "    shap.waterfall_plot(\n",
    "        explainer.expected_value if hasattr(explainer, 'expected_value') else 0,\n",
    "        shap_values_plot[non_survivor_example],\n",
    "        X_sample.iloc[non_survivor_example] if isinstance(X_sample, pd.DataFrame) else X_sample[non_survivor_example],\n",
    "        feature_names=X_train_scaled.columns if isinstance(X_train_scaled, pd.DataFrame) else None\n",
    "    )\n",
    "\n",
    "axes[1].set_title(f'SHAP Waterfall: Predicted Non-Survivor', fontsize=12, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"INTERPRETING WATERFALL PLOTS:\")\n",
    "print(\"=\"*70)\n",
    "print(\"â€¢ E[f(x)] = Expected model output (baseline prediction)\")\n",
    "print(\"â€¢ Red bars = Features pushing prediction UP (toward survival)\")\n",
    "print(\"â€¢ Blue bars = Features pushing prediction DOWN (toward death)\")  \n",
    "print(\"â€¢ f(x) = Final model output for this passenger\")\n",
    "print(\"=\"*70)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37537ed3",
   "metadata": {},
   "source": [
    "### 9.5 SHAP Dependence Plots - Feature Interactions\n",
    "\n",
    "Dependence plots show how a single feature affects predictions across its range, and potentially how it interacts with another feature.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c3e2f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature names\n",
    "if isinstance(X_train_scaled, pd.DataFrame):\n",
    "    feature_names = X_train_scaled.columns.tolist()\n",
    "    \n",
    "    # Find most important features for dependence plots\n",
    "    # Get feature importances from the model\n",
    "    if hasattr(model_for_shap, 'feature_importances_'):\n",
    "        feature_importance = pd.Series(\n",
    "            model_for_shap.feature_importances_,\n",
    "            index=feature_names\n",
    "        ).sort_values(ascending=False)\n",
    "        \n",
    "        # Select top 2 most important features for dependence plots\n",
    "        top_features = feature_importance.head(2).index.tolist()\n",
    "        \n",
    "        print(f\"\\nTop 2 features for dependence plots: {top_features}\")\n",
    "        \n",
    "        # Plot dependence for top 2 most important features\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(16, 5))\n",
    "        \n",
    "        for i, feat in enumerate(top_features):\n",
    "            feat_idx = feature_names.index(feat)\n",
    "            plt.sca(axes[i])\n",
    "            shap.dependence_plot(\n",
    "                feat_idx,\n",
    "                shap_values_plot,\n",
    "                X_sample,\n",
    "                feature_names=feature_names,\n",
    "                show=False\n",
    "            )\n",
    "            axes[i].set_title(f'SHAP Dependence: {feat}', fontsize=12, fontweight='bold')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"INTERPRETING DEPENDENCE PLOTS:\")\n",
    "        print(\"=\"*70)\n",
    "        print(\"â€¢ X-axis: Feature value\")\n",
    "        print(\"â€¢ Y-axis: SHAP value (impact on prediction)\")\n",
    "        print(\"â€¢ Color: Another feature that interacts with this one\")\n",
    "        print(\"â€¢ Patterns reveal how feature affects survival prediction\")\n",
    "        print(\"=\"*70)\n",
    "    else:\n",
    "        print(\"Model does not have feature_importances_ attribute\")\n",
    "else:\n",
    "    print(\"Dependence plots require feature names (DataFrame input)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b95a4efe",
   "metadata": {},
   "source": [
    "## 10. Production ML Pipeline\n",
    "\n",
    "For deployment, we've created a production-ready sklearn Pipeline in `src/pipeline/train_pipeline.py` that:\n",
    "- Encapsulates all preprocessing steps\n",
    "- Ensures reproducibility\n",
    "- Simplifies deployment\n",
    "- Prevents data leakage\n",
    "\n",
    "Let's demonstrate the pipeline approach alongside our notebook analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aed7915",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: This cell is commented out as the old pipeline has been replaced.\n",
    "# For production pipeline, use scripts/run_training.py instead.\n",
    "# See RESTRUCTURE_SUMMARY.md for details on the new structure.\n",
    "\n",
    "# # Import production pipeline\n",
    "# from src.pipeline.train_pipeline import TitanicTrainingPipeline, train_and_evaluate_pipeline\n",
    "\n",
    "# print(\"=\"*70)\n",
    "# print(\"PRODUCTION PIPELINE DEMONSTRATION\")\n",
    "# print(\"=\"*70)\n",
    "\n",
    "# # Initialize pipeline with optimized XGBoost parameters\n",
    "# production_pipeline = TitanicTrainingPipeline(random_state=config.RANDOM_SEED)\n",
    "\n",
    "# # Train pipeline\n",
    "# print(\"\\nTraining production pipeline...\")\n",
    "# pipeline_results = production_pipeline.train(train, train['Survived'], cv_folds=5)\n",
    "\n",
    "# print(\"\\n\" + \"=\"*70)\n",
    "# print(\"PIPELINE VS NOTEBOOK COMPARISON:\")\n",
    "# print(\"=\"*70)\n",
    "# print(f\"Notebook Best Model ({best_model_name}):\")\n",
    "# print(f\"  Cross-Val Score: {best_model_score:.4f}\")\n",
    "# print(f\"\\nProduction Pipeline:\")\n",
    "# print(f\"  Cross-Val Score: {pipeline_results['cv_mean']:.4f}\")\n",
    "# print(f\"  Std Dev: {pipeline_results['cv_std']:.4f}\")\n",
    "# print(\"=\"*70)\n",
    "\n",
    "# # Generate predictions with pipeline\n",
    "# pipeline_predictions = production_pipeline.predict(test)\n",
    "\n",
    "# # Compare predictions\n",
    "# agreement = np.mean(pipeline_predictions == best_model_preds)\n",
    "# print(f\"\\nPrediction Agreement: {agreement:.2%}\")\n",
    "# print(f\"  (How often both approaches agree on the same prediction)\")\n",
    "\n",
    "# # Save production pipeline\n",
    "# print(\"\\nSaving production pipeline...\")\n",
    "# production_pipeline.save('titanic_production_pipeline.pkl', 'models')\n",
    "\n",
    "# print(\"\\nâœ“ Production pipeline ready for deployment!\")\n",
    "# print(\"  Load with: TitanicTrainingPipeline.load('titanic_production_pipeline.pkl')\")\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"PRODUCTION PIPELINE - NEW STRUCTURE\")\n",
    "print(\"=\"*70)\n",
    "print(\"The production pipeline has been moved to a new modular structure.\")\n",
    "print(\"\\nTo train the model, run:\")\n",
    "print(\"  python scripts/run_training.py\")\n",
    "print(\"\\nTo make predictions, run:\")\n",
    "print(\"  python scripts/run_inference.py --input data/raw/test.csv\")\n",
    "print(\"\\nTo start the Flask API, run:\")\n",
    "print(\"  python scripts/run_api.py --port 5000\")\n",
    "print(\"\\nSee RESTRUCTURE_SUMMARY.md for complete documentation.\")\n",
    "print(\"=\"*70)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Titanic-Machine-Learning-from-Disaster",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
